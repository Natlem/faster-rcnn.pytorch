{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:42.641474Z",
     "start_time": "2018-05-03T19:22:42.606670Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:45.165985Z",
     "start_time": "2018-05-03T19:22:42.851773Z"
    }
   },
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from scipy.misc import imread\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "from model.nms.nms_wrapper import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.utils.blob import im_list_to_blob\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:45.217768Z",
     "start_time": "2018-05-03T19:22:45.169064Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "    im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "    blob (ndarray): a data blob holding an image pyramid\n",
    "    im_scale_factors (list): list of image scales (relative to im) used\n",
    "      in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= cfg.PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in cfg.TEST.SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n",
    "            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:45.242767Z",
     "start_time": "2018-05-03T19:22:45.219759Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_from_file(\"cfgs/vgg16.yml\")\n",
    "set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "cfg_from_list(set_cfgs)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "load_model_path = os.path.join(\"models/res101/holly/good_models/faster_rcnn_1_1_117000.pth\")\n",
    "holly_classes = np.asarray(['__background__', 'head'])\n",
    "isCuda = True\n",
    "cfg.CUDA = isCuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:45.313496Z",
     "start_time": "2018-05-03T19:22:45.244455Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getDiff(old_tensor, new_tensor, seq, detail=False):\n",
    "    total_difference = 0\n",
    "    diff = 0\n",
    "    original = 0  \n",
    "    if \"Conv2d\" in str(old_tensor) and not hasattr(old_tensor, \"__getitem__\") :\n",
    "        old_shape = old_tensor.weight.shape[0]\n",
    "        new_shape = new_tensor.weight.shape[0]\n",
    "        diff = old_shape - new_shape\n",
    "        original += old_shape\n",
    "        total_difference += diff\n",
    "        if detail:\n",
    "            print(str(e_old) + \"Difference = {}\".format(diff))\n",
    "    elif \"Sequential\" in str(old_tensor) and hasattr(old_tensor, \"__getitem__\"):\n",
    "        for j,b in enumerate(old_tensor):\n",
    "            bn_old = vars(b)\n",
    "            bn_new = vars(new_tensor[j])\n",
    "            convs_old = bn_old[\"_modules\"]\n",
    "            convs_new = bn_new[\"_modules\"]\n",
    "            for k,v in convs_old.items():\n",
    "                if \"conv\" in k:\n",
    "                    old_shape = v.weight.shape[0]\n",
    "                    original += old_shape\n",
    "                    new_shape = convs_new[k].weight.shape[0]\n",
    "                    diff = old_shape - new_shape\n",
    "                    if detail:\n",
    "                        print(\"S {}/ B {}/ {} Difference = {}\".format(seq, j, k, diff))\n",
    "                    total_difference += diff\n",
    "    return original, total_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:45.343272Z",
     "start_time": "2018-05-03T19:22:45.315946Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_cuda(isCuda, tensor):\n",
    "    if isCuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:47.837736Z",
     "start_time": "2018-05-03T19:22:45.346622Z"
    },
    "code_folding": [
     0,
     3,
     12,
     25,
     37,
     47,
     59,
     69,
     79,
     88,
     94,
     115,
     151,
     181
    ]
   },
   "outputs": [],
   "source": [
    "def getNumpy(tensor):\n",
    "    return tensor.data.cpu().numpy()\n",
    "\n",
    "def sortFilters(filters):\n",
    "    #Sort filters by L1 Norm\n",
    "    c = filters.reshape(-1, filters.shape[0])\n",
    "    c = np.linalg.norm(c, ord=1, axis=0)\n",
    "    d = np.zeros([2, c.shape[0]])\n",
    "    i = np.argsort(c)\n",
    "    d[0,:] = i\n",
    "    d[1,:] = c[i]\n",
    "    return d\n",
    "def removeFilters(filters, percent, threshold=None):\n",
    "    #Remove filters from numpy \n",
    "    t = threshold\n",
    "    if threshold is None:\n",
    "        t = np.median(filters[1,:]) - np.std(filters[1,:])\n",
    "    mask = filters[1, :] > t\n",
    "    numRemoved_1 = mask.sum()\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - percent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def removePercent(filters, KeepPercent, PrunePercent):\n",
    "    #Remove filters from numpy \n",
    "    numOfRemove = filters.shape[1] * PrunePercent / 100.\n",
    "    numRemoved_1 = int(numOfRemove)\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - KeepPercent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    print(\"NumOfZeros:\")\n",
    "    print(numOfZeros)\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def sortRecoverFilters(l1Array, filters):\n",
    "    #Recover the order of the pruned filters\n",
    "    b = np.argsort(l1Array[0, :])\n",
    "    c = l1Array[:,b]\n",
    "    c = c[0,:].astype(int)\n",
    "    shape = np.asarray(filters.shape)\n",
    "    shape[0] = c.shape[0]\n",
    "    newFilters = np.zeros(shape)\n",
    "    newFilters = filters[c,:,:,:]\n",
    "    return newFilters\n",
    "def sortRecoverBatch(bn_tensor, index):\n",
    "    #Batch pruning \n",
    "    index.sort()\n",
    "    bn_rmean = bn_tensor.running_mean.cpu().numpy()\n",
    "    bn_tensor.running_mean = torch.from_numpy(bn_rmean[index]).float().cuda()\n",
    "    bn_rvar = bn_tensor.running_var.cpu().numpy()\n",
    "    bn_tensor.running_var = torch.from_numpy(bn_rvar[index]).float().cuda()\n",
    "    bn_weight = bn_tensor.weight.data.cpu().numpy()\n",
    "    bn_tensor.weight.data = torch.from_numpy(bn_weight[index]).float().cuda()\n",
    "    bn_bias = bn_tensor.bias.data.cpu().numpy()\n",
    "    bn_tensor.bias.data = torch.from_numpy(bn_bias[index]).float().cuda()\n",
    "\n",
    "def pruneConvLayers(tensor, percent = 10, threshold = None):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removeFilters(d, percent, threshold)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvPercent(tensor, KeepPercent = 10, PrunePercent=10):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removePercent(d, KeepPercent, PrunePercent)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvLowest(tensor, numKeep):\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = d[:,0:numKeep]\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvWithIndexes(tensor, rindexes):\n",
    "    #Prune out channels of convolutional layers using indexes\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    tensor.weight.data = torch.from_numpy(filters[rindexes]).float().cuda()\n",
    "    tensor.out_channels = tensor.weight.data.shape[0]\n",
    "    return tensor, rindexes\n",
    "def pruneNextLayer(nextLayerTensor, prevOutput, rindexes=None):\n",
    "    #Prune input channels of everything\n",
    "    if \"BatchNorm\" in str(nextLayerTensor):\n",
    "        nextLayerTensor.num_features = prevOutput\n",
    "        sortRecoverBatch(nextLayerTensor, rindexes)\n",
    "    elif \"Conv2d\" in str(nextLayerTensor):\n",
    "        if nextLayerTensor.weight.shape[1] == prevOutput:\n",
    "            return\n",
    "        nextLayerTensor.in_channels = prevOutput\n",
    "        nextConvWeight = getNumpy(nextLayerTensor.weight)\n",
    "        if not rindexes is None:\n",
    "            c = nextConvWeight[:,rindexes,:,:]\n",
    "        else:\n",
    "            c = nextConvWeight\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(c).float().cuda()\n",
    "    elif \"Linear\" in str(nextLayerTensor):\n",
    "        n = getNumpy(nextLayerTensor.weight)\n",
    "        fc = n[:,rindexes]\n",
    "        nextLayerTensor.in_features = fc.shape[1]\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(fc).float().cuda()\n",
    "    return nextLayerTensor\n",
    "def prune_bottleneck(bottlenecks, prevOutput, rindexesX, keeplast=False):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, prevOutput, rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv1) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv2) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], prevOutput, rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvLayers(b_neck[0]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "        else:\n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "\n",
    "def prune_bottleneck_percent(bottlenecks, rindexesX, percents):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, len(rindexesX), rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv1, percents[0]) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv2, percents[1]) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], len(rindexesX), rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvPercent(b_neck[0], percents[2]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "        else:\n",
    "            lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "\n",
    "def pruneCBias(convTensor, rindexes):\n",
    "    convT = convTensor\n",
    "    bias_data = convT.bias.data.cpu().numpy()\n",
    "    convT.bias.data = torch.from_numpy(bias_data[rindexes]).float().cuda()\n",
    "    return convT, rindexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:22:47.929397Z",
     "start_time": "2018-05-03T19:22:47.845507Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pruneEverything():\n",
    "    percent = 10\n",
    "    prevIndex = 0\n",
    "    prevOutput = 3\n",
    "    rindexes = None\n",
    "    for i,e in enumerate(fasterRCNN.RCNN_base):\n",
    "        currentTensor = e\n",
    "        if \"Conv2d\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.in_channels:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput)\n",
    "            currentTensor, rindexes = pruneConvLayers(currentTensor, percent)\n",
    "            prevOutput = currentTensor.out_channels\n",
    "\n",
    "        elif \"BatchNorm\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.num_features:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput, rindexes)           \n",
    "            prevOutput = currentTensor.num_features\n",
    "        elif hasattr(currentTensor, \"__getitem__\"):\n",
    "            b, prevOutput, rindexes = prune_bottleneck(currentTensor, prevOutput, rindexes)\n",
    "\n",
    "        prevIndex = i\n",
    "        prevTensor = currentTensor\n",
    "\n",
    "    #Prune RoI pooling and FC classifiers\n",
    "    b = pruneNextLayer(fasterRCNN.RCNN_rpn.RPN_Conv, prevOutput, rindexes)\n",
    "    b, p, r = prune_bottleneck(fasterRCNN.RCNN_top[0], prevOutput, rindexes)\n",
    "    fc = pruneNextLayer(fasterRCNN.RCNN_bbox_pred, p, r)\n",
    "    cls_score = pruneNextLayer(fasterRCNN.RCNN_cls_score, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:24.945121Z",
     "start_time": "2018-05-03T19:22:47.938060Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_path = \"pth_train_dir/vgg16/finished_models/vgg16_test_2_60000.pth\"\n",
    "fasterRCNN = vgg16(holly_classes, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()\n",
    "checkpoint = torch.load(load_model_path)\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:25.003683Z",
     "start_time": "2018-05-03T19:23:24.949646Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def reload_model(weightsPTH):\n",
    "    fasterRCNN = vgg16(holly_classes, pretrained=False, class_agnostic=False)\n",
    "    fasterRCNN.create_architecture()\n",
    "    checkpoint = torch.load(weightsPTH)\n",
    "    fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "    fasterRCNN.cuda()\n",
    "    if 'pooling_mode' in checkpoint.keys():\n",
    "        cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "    return fasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:25.170039Z",
     "start_time": "2018-05-03T19:23:25.007278Z"
    }
   },
   "outputs": [],
   "source": [
    "if isCuda:\n",
    "    fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:26.788569Z",
     "start_time": "2018-05-03T19:23:25.173604Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evalNetwork(modelFR, out=False):\n",
    "    modelFR.eval()\n",
    "    \n",
    "    cfg.TRAIN.USE_FLIPPED = False\n",
    "    imdb_name = \"holly_test\"\n",
    "    imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name, False)\n",
    "    imdb.competition_mode(on=True)\n",
    "    \n",
    "    save_name = 'faster_rcnn_10'\n",
    "    num_images = len(imdb.image_index)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "               for _ in range(imdb.num_classes)]\n",
    "\n",
    "    output_dir = get_output_dir(imdb, save_name)\n",
    "\n",
    "    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                            imdb.num_classes, training=False, normalize = False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=0,\n",
    "                                pin_memory=True)\n",
    "    output_dir = \"/tmp/outTest\"\n",
    "    vis = False\n",
    "    thresh = 0.0\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    num_images = len(imdb.image_index)\n",
    "\n",
    "    start = time.time()\n",
    "    max_per_image = 100\n",
    "\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        data = next(data_iter)\n",
    "        im_dataC.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_infoC.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxesC.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxesC.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "        rois_label = modelFR(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        if cfg.TEST.BBOX_REG:\n",
    "              # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "              # Optionally normalize targets by a precomputed mean and stdev\n",
    "                if False:#args.class_agnostic:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4)\n",
    "                else:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_infoC.data, 1)\n",
    "        else:\n",
    "              # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "        pred_boxes /= data[1][0][2]\n",
    "\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "        if vis:\n",
    "            im = cv2.imread(imdb.image_path_at(i))\n",
    "            im2show = np.copy(im)\n",
    "        for j in range(1, imdb.num_classes):\n",
    "            inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "              # if there is det\n",
    "            if inds.numel() > 0:\n",
    "                cls_scores = scores[:,j][inds]\n",
    "                _, order = torch.sort(cls_scores, 0, True)\n",
    "                if False:#args.class_agnostic:\n",
    "                    cls_boxes = pred_boxes[inds, :]\n",
    "                else:\n",
    "                    cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "                cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "                # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "                cls_dets = cls_dets[order]\n",
    "                keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "                cls_dets = cls_dets[keep.view(-1).long()]\n",
    "                if vis:\n",
    "                    im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "                all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "            else:\n",
    "                all_boxes[j][i] = empty_array\n",
    "\n",
    "          # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "            image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                        for j in range(1, imdb.num_classes)])\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "                for j in range(1, imdb.num_classes):\n",
    "                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "          .format(i + 1, num_images, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if vis:\n",
    "            cv2.imwrite('result.png', im2show)\n",
    "            pdb.set_trace()\n",
    "\n",
    "    with open(det_file, 'wb+') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    ap = imdb.evaluate_detections(all_boxes, output_dir, out)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:26.813738Z",
     "start_time": "2018-05-03T19:23:26.790745Z"
    }
   },
   "outputs": [],
   "source": [
    "def getFeaturesMap(self, input, output):\n",
    "    gpfm = F.avg_pool2d(Variable(output.data.cpu()), kernel_size=(output.data.size()[2], output.data.size()[3])).data.numpy().reshape((1,self.out_channels))\n",
    "    if len(allFM[self]) == 0:\n",
    "        allFM[self] = gpfm\n",
    "    else:\n",
    "        allFM[self] = np.concatenate((allFM[self], gpfm), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:26.980297Z",
     "start_time": "2018-05-03T19:23:26.816415Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run1Forward(model, imagePath):\n",
    "    model.eval()\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    im_file = os.path.join(imagePath)\n",
    "            # im = cv2.imread(im_file)\n",
    "    im_in = np.array(imread(im_file))\n",
    "    if len(im_in.shape) == 2:\n",
    "        im_in = im_in[:,:,np.newaxis]\n",
    "        im_in = np.concatenate((im_in,im_in,im_in), axis=2)\n",
    "          # rgb -> bgr\n",
    "    im = im_in[:,:,::-1]\n",
    "    blobs, im_scales = _get_image_blob(im)\n",
    "    assert len(im_scales) == 1, \"Only single-image batch implemented\"\n",
    "    im_blob = blobs\n",
    "    im_info_np = np.array([[im_blob.shape[1], im_blob.shape[2], im_scales[0]]], dtype=np.float32)\n",
    "\n",
    "    im_data_pt = torch.from_numpy(im_blob)\n",
    "    im_data_pt = im_data_pt.permute(0, 3, 1, 2)\n",
    "    im_info_pt = torch.from_numpy(im_info_np)\n",
    "\n",
    "    im_dataC.data.resize_(im_data_pt.size()).copy_(im_data_pt)\n",
    "    im_infoC.data.resize_(im_info_pt.size()).copy_(im_info_pt)\n",
    "    gt_boxesC.data.resize_(1, 1, 5).zero_()\n",
    "    num_boxesC.data.resize_(1).zero_()\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    det_tic = time.time()\n",
    "\n",
    "    rois, cls_prob, bbox_pred, \\\n",
    "    rpn_loss_cls, rpn_loss_box, \\\n",
    "    RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "    rois_label = model(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "    scores = cls_prob.data\n",
    "    boxes = rois.data[:, :, 1:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T20:03:34.549815Z",
     "start_time": "2018-04-05T20:03:34.492869Z"
    }
   },
   "source": [
    "allFM[fasterRCNN.RCNN_base[0]] = []"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T20:03:34.829931Z",
     "start_time": "2018-04-05T20:03:34.795594Z"
    }
   },
   "source": [
    "fasterRCNN.RCNN_base[0].register_forward_hook(getFeaturesMap)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T20:03:38.187851Z",
     "start_time": "2018-04-05T20:03:37.856250Z"
    }
   },
   "source": [
    "run1Forward(fasterRCNN, \"images/img_head.jpeg\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T20:05:07.101528Z",
     "start_time": "2018-04-05T20:05:07.061074Z"
    }
   },
   "source": [
    "allFM[fasterRCNN.RCNN_base[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:23:27.015988Z",
     "start_time": "2018-05-03T19:23:26.983685Z"
    }
   },
   "outputs": [],
   "source": [
    "hooklist = []\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e):\n",
    "        hooklist.append(e.register_forward_hook(getFeaturesMap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:32:45.594841Z",
     "start_time": "2018-05-03T19:23:27.019792Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/anaconda2/envs/py3ml/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    }
   ],
   "source": [
    "pruneText = \"data/HF_combined/HF_combined/ImageSets/Main/prune.txt\"\n",
    "imgFolder = \"data/HF_combined/HF_combined/JPEGImages/\"\n",
    "img_extension = \".jpg\"\n",
    "allFM = {}\n",
    "numOfImages = 1000\n",
    "\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e):\n",
    "        allFM[e] = []\n",
    "\n",
    "with open(pruneText, 'r') as f:\n",
    "    i = 0\n",
    "    for imgName in f:\n",
    "        i += 1\n",
    "        imgPath = os.path.join(imgFolder, imgName.rstrip() + img_extension)\n",
    "        run1Forward(fasterRCNN , imgPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:32:45.813586Z",
     "start_time": "2018-05-03T19:32:45.598044Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSortedIndex(convTensor, allFM):\n",
    "    Hi_0 = []\n",
    "    for i in range(allFM[convTensor].shape[1]):\n",
    "        hist, bin_edges = np.histogram(allFM[convTensor][i,:], bins=10)\n",
    "        p = hist / allFM[convTensor].shape[0]\n",
    "        p = p[np.nonzero(p)]\n",
    "        Hi_0.append((p * np.log(p)).sum())\n",
    "\n",
    "    Hi_0 = np.asarray(Hi_0)\n",
    "    d = np.zeros([2, Hi_0.shape[0]])\n",
    "    i = np.argsort(Hi_0)\n",
    "    return i.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:32:46.770644Z",
     "start_time": "2018-05-03T19:32:45.815841Z"
    }
   },
   "outputs": [],
   "source": [
    "prevOutput = 3\n",
    "prevTensor = None\n",
    "compressionRate = 70\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e) and i != 28:\n",
    "        if prevOutput != e.in_channels:\n",
    "            e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        index = getSortedIndex(e, allFM)\n",
    "        num2Removes = int(index.shape[0] * compressionRate / 100.)\n",
    "        _, rindexes = pruneConvWithIndexes(e, index[num2Removes:])\n",
    "        pruneCBias(e, rindexes)\n",
    "        prevOutput = e.out_channels\n",
    "        prevTensor = e\n",
    "    if i == 28:\n",
    "        e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T01:47:51.299743Z",
     "start_time": "2018-04-19T01:47:51.275010Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in hooklist:\n",
    "    i.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T01:47:51.632322Z",
     "start_time": "2018-04-19T01:47:51.602633Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (17): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace)\n",
       "  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (24): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace)\n",
       "  (26): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace)\n",
       "  (28): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.RCNN_base"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T03:01:34.714667Z",
     "start_time": "2018-04-06T03:01:34.676392Z"
    }
   },
   "source": [
    "#### Hi_0 = []\n",
    "for i in range(allFM[fasterRCNN.RCNN_base[0]].shape[1]):\n",
    "    hist, bin_edges = np.histogram(allFM[fasterRCNN.RCNN_base[0]][i,:], bins=10)\n",
    "    p = hist / allFM[fasterRCNN.RCNN_base[0]].shape[0]\n",
    "    p = p[np.nonzero(p)]\n",
    "    Hi_0.append((p * np.log(p)).sum())\n",
    "\n",
    "Hi_0 = np.asarray(Hi_0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T03:01:35.384625Z",
     "start_time": "2018-04-06T03:01:35.342395Z"
    }
   },
   "source": [
    "#### a = np.asarray(Hi_0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T03:01:35.707970Z",
     "start_time": "2018-04-06T03:01:35.679223Z"
    }
   },
   "source": [
    "d = np.zeros([2, a.shape[0]])\n",
    "i = np.argsort(a)\n",
    "d[0,:] = i\n",
    "d[1,:] = a[i]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T03:01:37.214521Z",
     "start_time": "2018-04-06T03:01:37.156922Z"
    }
   },
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-06T01:49:15.138535Z",
     "start_time": "2018-04-06T01:49:14.893634Z"
    }
   },
   "source": [
    "hist, bin_edges = np.histogram(allFM[fasterRCNN.RCNN_base[0]][0,:], bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T01:41:09.328748Z",
     "start_time": "2018-04-07T01:39:28.898258Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(allFM, open(\"/tmp/allFM.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-07T02:17:23.560352Z",
     "start_time": "2018-04-07T02:14:06.911777Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 1/1293 0.076s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.071s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.003003609493902607"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-19T01:48:14.904Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_50_entropy.p')\n",
    "torch.save(fasterRCNN, save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T19:04:54.442518Z",
     "start_time": "2018-04-21T19:04:48.900120Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_50_entropy.p')\n",
    "fasterRCNN = torch.load(save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T19:53:58.291225Z",
     "start_time": "2018-04-21T19:53:47.631176Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pth_train_dir/vgg16/pruned_entropy_50/vgg16/holly/faster_rcnn_1_1_170000.pth\")\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T19:56:16.621551Z",
     "start_time": "2018-04-21T19:53:58.293622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 2/1293 0.044s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.038s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7249153714969145"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T14:32:05.669769Z",
     "start_time": "2018-04-05T14:32:05.209766Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/allFM.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e7b6415e38a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mallFM\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/allFM.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/allFM.p'"
     ]
    }
   ],
   "source": [
    "allFM =  pickle.load(open(\"/tmp/allFM.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
