{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:37.423145Z",
     "start_time": "2018-05-14T19:54:30.211162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from scipy.misc import imread\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "from model.nms.nms_wrapper import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.utils.blob import im_list_to_blob\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:37.502323Z",
     "start_time": "2018-05-14T19:54:37.424734Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "    im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "    blob (ndarray): a data blob holding an image pyramid\n",
    "    im_scale_factors (list): list of image scales (relative to im) used\n",
    "      in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= cfg.PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in cfg.TEST.SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n",
    "            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:37.580925Z",
     "start_time": "2018-05-14T19:54:37.503925Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_from_file(\"cfgs/vgg16.yml\")\n",
    "set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "cfg_from_list(set_cfgs)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "holly_classes = np.asarray(['__background__', 'head'])\n",
    "isCuda = True\n",
    "cfg.CUDA = isCuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:37.628889Z",
     "start_time": "2018-05-14T19:54:37.582825Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_cuda(isCuda, tensor):\n",
    "    if isCuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:37.684463Z",
     "start_time": "2018-05-14T19:54:37.630684Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class sampler(Sampler):\n",
    "    def __init__(self, train_size, batch_size):\n",
    "        self.num_data = train_size\n",
    "        self.num_per_batch = int(train_size / batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
    "        self.leftover_flag = False\n",
    "        if train_size % batch_size:\n",
    "            self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "            self.leftover_flag = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
    "        self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
    "        self.rand_num_view = self.rand_num.view(-1)\n",
    "\n",
    "        if self.leftover_flag:\n",
    "            self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
    "\n",
    "        return iter(self.rand_num_view)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:54:39.227345Z",
     "start_time": "2018-05-14T19:54:37.686828Z"
    },
    "code_folding": [
     0,
     2,
     11,
     23,
     34,
     44,
     55,
     65,
     75,
     84,
     90,
     111,
     146,
     175
    ]
   },
   "outputs": [],
   "source": [
    "def getNumpy(tensor):\n",
    "    return tensor.data.cpu().numpy()\n",
    "def sortFilters(filters):\n",
    "    #Sort filters by L1 Norm\n",
    "    c = filters.reshape(-1, filters.shape[0])\n",
    "    c = np.linalg.norm(c, ord=1, axis=0)\n",
    "    d = np.zeros([2, c.shape[0]])\n",
    "    i = np.argsort(c)\n",
    "    d[0,:] = i\n",
    "    d[1,:] = c[i]\n",
    "    return d\n",
    "def removeFilters(filters, percent, threshold=None):\n",
    "    #Remove filters from numpy \n",
    "    t = threshold\n",
    "    if threshold is None:\n",
    "        t = np.median(filters[1,:]) - np.std(filters[1,:])\n",
    "    mask = filters[1, :] > t\n",
    "    numRemoved_1 = mask.sum()\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - percent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "def removePercent(filters, KeepPercent, PrunePercent):\n",
    "    #Remove filters from numpy \n",
    "    numOfRemove = filters.shape[1] * PrunePercent / 100.\n",
    "    numRemoved_1 = int(numOfRemove)\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - KeepPercent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    print(\"NumOfZeros:\")\n",
    "    print(numOfZeros)\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "def sortRecoverFilters(l1Array, filters):\n",
    "    #Recover the order of the pruned filters\n",
    "    b = np.argsort(l1Array[0, :])\n",
    "    c = l1Array[:,b]\n",
    "    c = c[0,:].astype(int)\n",
    "    shape = np.asarray(filters.shape)\n",
    "    shape[0] = c.shape[0]\n",
    "    newFilters = np.zeros(shape)\n",
    "    newFilters = filters[c,:,:,:]\n",
    "    return newFilters\n",
    "def sortRecoverBatch(bn_tensor, index):\n",
    "    #Batch pruning \n",
    "    index.sort()\n",
    "    bn_rmean = bn_tensor.running_mean.cpu().numpy()\n",
    "    bn_tensor.running_mean = torch.from_numpy(bn_rmean[index]).float().cuda()\n",
    "    bn_rvar = bn_tensor.running_var.cpu().numpy()\n",
    "    bn_tensor.running_var = torch.from_numpy(bn_rvar[index]).float().cuda()\n",
    "    bn_weight = bn_tensor.weight.data.cpu().numpy()\n",
    "    bn_tensor.weight.data = torch.from_numpy(bn_weight[index]).float().cuda()\n",
    "    bn_bias = bn_tensor.bias.data.cpu().numpy()\n",
    "    bn_tensor.bias.data = torch.from_numpy(bn_bias[index]).float().cuda()\n",
    "def pruneConvLayers(tensor, percent = 10, threshold = None):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removeFilters(d, percent, threshold)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvPercent(tensor, KeepPercent = 10, PrunePercent=10):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removePercent(d, KeepPercent, PrunePercent)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvLowest(tensor, numKeep):\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = d[:,0:numKeep]\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvWithIndexes(tensor, rindexes):\n",
    "    #Prune out channels of convolutional layers using indexes\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    tensor.weight.data = torch.from_numpy(filters[rindexes]).float().cuda()\n",
    "    tensor.out_channels = tensor.weight.data.shape[0]\n",
    "    return tensor, rindexes\n",
    "def pruneNextLayer(nextLayerTensor, prevOutput, rindexes=None):\n",
    "    #Prune input channels of everything\n",
    "    if \"BatchNorm\" in str(nextLayerTensor):\n",
    "        nextLayerTensor.num_features = prevOutput\n",
    "        sortRecoverBatch(nextLayerTensor, rindexes)\n",
    "    elif \"Conv2d\" in str(nextLayerTensor):\n",
    "        if nextLayerTensor.weight.shape[1] == prevOutput:\n",
    "            return\n",
    "        nextLayerTensor.in_channels = prevOutput\n",
    "        nextConvWeight = getNumpy(nextLayerTensor.weight)\n",
    "        if not rindexes is None:\n",
    "            c = nextConvWeight[:,rindexes,:,:]\n",
    "        else:\n",
    "            c = nextConvWeight\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(c).float().cuda()\n",
    "    elif \"Linear\" in str(nextLayerTensor):\n",
    "        n = getNumpy(nextLayerTensor.weight)\n",
    "        fc = n[:,rindexes]\n",
    "        nextLayerTensor.in_features = fc.shape[1]\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(fc).float().cuda()\n",
    "    return nextLayerTensor\n",
    "def prune_bottleneck(bottlenecks, prevOutput, rindexesX, keeplast=False):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, prevOutput, rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv1) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv2) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], prevOutput, rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvLayers(b_neck[0]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "        else:\n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "def prune_bottleneck_percent(bottlenecks, rindexesX, percents):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, len(rindexesX), rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv1, percents[0]) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv2, percents[1]) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], len(rindexesX), rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvPercent(b_neck[0], percents[2]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "        else:\n",
    "            lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "def pruneCBias(convTensor, rindexes):\n",
    "    convT = convTensor\n",
    "    bias_data = convT.bias.data.cpu().numpy()\n",
    "    convT.bias.data = torch.from_numpy(bias_data[rindexes]).float().cuda()\n",
    "    return convT, rindexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:00.045141Z",
     "start_time": "2018-05-14T19:54:39.230583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__prune` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__prune gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__prune_gt_roidb.pkl\n",
      "done\n",
      "before filtering, there are 1000 images...\n",
      "after filtering, there are 1000 images...\n"
     ]
    }
   ],
   "source": [
    "imdb_name = \"holly_prune\"\n",
    "imdb, roidb, ratio_list,ratio_index = combined_roidb(imdb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:00.249630Z",
     "start_time": "2018-05-14T19:55:00.047732Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def runModel(model, imdb, roidb, ratio_list, ratio_index):\n",
    "    batch_size = 1\n",
    "    cfg.TRAIN.USE_FLIPPED = True\n",
    "    cfg.USE_GPU_NMS = True\n",
    "    start_epoch = 0\n",
    "    max_epochs = 1\n",
    "    lr_decay_step = 5\n",
    "    lr_decay_gamma = 0.1\n",
    "    disp_interval = 100\n",
    "    output_dir = \"./pth_train_dir/vgg16/pruned\"\n",
    "    class_agnostic = False\n",
    "    session = 1\n",
    "    train_size = len(roidb)\n",
    "    lr = 0.0001 #cfg.TRAIN.LEARNING_RATE\n",
    "    \n",
    "    sampler_batch = sampler(train_size, batch_size)\n",
    "    dataset_train = roibatchLoader(roidb, ratio_list, ratio_index, batch_size, imdb.num_classes, training=True)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=sampler_batch, num_workers=1)\n",
    "    im_dataT = torch.FloatTensor(1)\n",
    "    im_infoT = torch.FloatTensor(1)\n",
    "    num_boxesT = torch.LongTensor(1)\n",
    "    gt_boxesT = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataT = make_cuda(isCuda, im_dataT)\n",
    "    im_infoT = make_cuda(isCuda, im_infoT)\n",
    "    num_boxesT = make_cuda(isCuda, num_boxesT)\n",
    "    gt_boxesT = make_cuda(isCuda, gt_boxesT)\n",
    "\n",
    "      # make variable\n",
    "    im_dataT = Variable(im_dataT)\n",
    "    im_infoT = Variable(im_infoT)\n",
    "    num_boxesT = Variable(num_boxesT)\n",
    "    gt_boxesT = Variable(gt_boxesT)\n",
    "\n",
    "    \n",
    "    params = []\n",
    "    for key, value in dict(model.named_parameters()).items():\n",
    "        value.requires_grad = True\n",
    "\n",
    "    iters_per_epoch = int(train_size / batch_size)\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs + 1):\n",
    "        # setting to train mode\n",
    "        model.train()\n",
    "        loss_temp = 0\n",
    "\n",
    "        if epoch % (lr_decay_step + 1) == 0:\n",
    "            lr *= lr_decay_gamma\n",
    "\n",
    "        data_iter = iter(dataloader_train)\n",
    "        for step in range(iters_per_epoch):\n",
    "            data = next(data_iter)\n",
    "            \n",
    "            im_dataT.data.resize_(data[0].size()).copy_(data[0])\n",
    "            im_infoT.data.resize_(data[1].size()).copy_(data[1])\n",
    "            gt_boxesT.data.resize_(data[2].size()).copy_(data[2])\n",
    "            num_boxesT.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "            model.zero_grad()\n",
    "            rois, cls_prob, bbox_pred, \\\n",
    "            rpn_loss_cls, rpn_loss_box, \\\n",
    "            RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "            rois_label = model(im_dataT, im_infoT, gt_boxesT, num_boxesT)\n",
    "\n",
    "            loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
    "               + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n",
    "            loss_temp += loss.data[0]\n",
    "\n",
    "            # backward\n",
    "            loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:01.098720Z",
     "start_time": "2018-05-14T19:55:00.251823Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evalNetwork(modelFR, out=False):\n",
    "    modelFR.eval()\n",
    "    \n",
    "    cfg.TRAIN.USE_FLIPPED = False\n",
    "    imdb_name = \"holly_test\"\n",
    "    imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name, False)\n",
    "    imdb.competition_mode(on=True)\n",
    "    \n",
    "    save_name = 'faster_rcnn_10'\n",
    "    num_images = len(imdb.image_index)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "               for _ in range(imdb.num_classes)]\n",
    "\n",
    "    output_dir = get_output_dir(imdb, save_name)\n",
    "\n",
    "    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                            imdb.num_classes, training=False, normalize = False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=0,\n",
    "                                pin_memory=True)\n",
    "    output_dir = \"/tmp/outTest\"\n",
    "    vis = False\n",
    "    thresh = 0.0\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    num_images = len(imdb.image_index)\n",
    "\n",
    "    start = time.time()\n",
    "    max_per_image = 100\n",
    "\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        data = next(data_iter)\n",
    "        im_dataC.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_infoC.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxesC.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxesC.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "        rois_label = modelFR(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        if cfg.TEST.BBOX_REG:\n",
    "              # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "              # Optionally normalize targets by a precomputed mean and stdev\n",
    "                if False:#args.class_agnostic:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4)\n",
    "                else:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_infoC.data, 1)\n",
    "        else:\n",
    "              # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "        pred_boxes /= data[1][0][2]\n",
    "\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "        if vis:\n",
    "            im = cv2.imread(imdb.image_path_at(i))\n",
    "            im2show = np.copy(im)\n",
    "        for j in range(1, imdb.num_classes):\n",
    "            inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "              # if there is det\n",
    "            if inds.numel() > 0:\n",
    "                cls_scores = scores[:,j][inds]\n",
    "                _, order = torch.sort(cls_scores, 0, True)\n",
    "                if False:#args.class_agnostic:\n",
    "                    cls_boxes = pred_boxes[inds, :]\n",
    "                else:\n",
    "                    cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "                cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "                # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "                cls_dets = cls_dets[order]\n",
    "                keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "                cls_dets = cls_dets[keep.view(-1).long()]\n",
    "                if vis:\n",
    "                    im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "                all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "            else:\n",
    "                all_boxes[j][i] = empty_array\n",
    "\n",
    "          # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "            image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                        for j in range(1, imdb.num_classes)])\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "                for j in range(1, imdb.num_classes):\n",
    "                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "          .format(i + 1, num_images, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if vis:\n",
    "            cv2.imwrite('result.png', im2show)\n",
    "            pdb.set_trace()\n",
    "\n",
    "    with open(det_file, 'wb+') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    ap = imdb.evaluate_detections(all_boxes, output_dir, out)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:15.807890Z",
     "start_time": "2018-05-14T19:55:01.100589Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_path = \"pth_train_dir/vgg16/finished_models/vgg16_test_2_60000.pth\"\n",
    "fasterRCNN = vgg16(holly_classes, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()\n",
    "checkpoint = torch.load(load_model_path)\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:15.855061Z",
     "start_time": "2018-05-14T19:55:15.810341Z"
    }
   },
   "outputs": [],
   "source": [
    "lFM = {}\n",
    "filter_ranks = {}\n",
    "\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e):\n",
    "        lFM[e] = None\n",
    "        filter_ranks[e] = None\n",
    "\n",
    "def forward(self, input, output):\n",
    "    lFM[self] = output\n",
    "    \n",
    "def backward(self, grad_in, grad_out):\n",
    "    \n",
    "    val = lFM[self] * grad_out[0]\n",
    "    values = torch.sum(val, dim = 0, keepdim = True)\n",
    "    values = values.sum(dim=2, keepdim = True)\n",
    "    values = values.sum(dim=3, keepdim = True)[0, :, 0, 0].data\n",
    "    values = values / (lFM[self].size(0) * lFM[self].size(2) * lFM[self].size(3))\n",
    "    if filter_ranks[self] is None:\n",
    "        filter_ranks[self] = torch.FloatTensor(lFM[self].size(1)).zero_().cuda()\n",
    "    \n",
    "    lFM[self] = None\n",
    "    filter_ranks[self] += values\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:16.027826Z",
     "start_time": "2018-05-14T19:55:15.856685Z"
    }
   },
   "outputs": [],
   "source": [
    "fasterRCNN = fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:16.054731Z",
     "start_time": "2018-05-14T19:55:16.030045Z"
    }
   },
   "outputs": [],
   "source": [
    "hooklist = []\n",
    "for i in hooklist:\n",
    "    i.remove()\n",
    "\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e):\n",
    "        hooklist.append(e.register_forward_hook(forward))\n",
    "        hooklist.append(e.register_backward_hook(backward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:55:19.054639Z",
     "start_time": "2018-05-14T19:55:16.056762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    }
   ],
   "source": [
    "runModel(fasterRCNN, imdb, roidb, ratio_list, ratio_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T14:27:19.896382Z",
     "start_time": "2018-05-07T14:27:19.870955Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_ranks_per_layer(filter_ranks):\n",
    "    for i in filter_ranks:\n",
    "        v = torch.abs(filter_ranks[i])\n",
    "        v = v / np.sqrt(torch.sum(v * v))\n",
    "        filter_ranks[i] = v.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T14:27:19.918360Z",
     "start_time": "2018-05-07T14:27:19.898285Z"
    }
   },
   "outputs": [],
   "source": [
    "normalize_ranks_per_layer(filter_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T14:27:20.859029Z",
     "start_time": "2018-05-07T14:27:20.836226Z"
    }
   },
   "outputs": [],
   "source": [
    "def getSortedIndex(convTensor, filter_ranks):\n",
    "    ranks = filter_ranks[convTensor].numpy()\n",
    "    ranks = np.asarray(ranks)\n",
    "    i = np.argsort(ranks)\n",
    "    return i.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-07T14:27:22.153044Z",
     "start_time": "2018-05-07T14:27:22.130584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59,  2, 27, 56,  0, 52, 42, 40, 50, 46, 12, 17, 53, 31, 16, 43,  1,\n",
       "       33,  4, 45, 60, 30, 63,  7, 58,  6, 21, 19, 35, 24, 20, 38, 37, 29,\n",
       "       22, 55, 48, 34,  3, 10, 47, 54,  9, 11, 28, 26, 44, 25, 23, 57, 51,\n",
       "       62, 15, 39, 32, 14, 36, 41,  8, 18,  5, 13, 49, 61])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSortedIndex(fasterRCNN.RCNN_base[0], filter_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:32:07.591957Z",
     "start_time": "2018-04-30T16:32:07.437357Z"
    }
   },
   "outputs": [],
   "source": [
    "prevOutput = 3\n",
    "prevTensor = None\n",
    "compressionRate = 70\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e) and i != 28:\n",
    "        if prevOutput != e.in_channels:\n",
    "            e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        index = getSortedIndex(e, filter_ranks)\n",
    "        num2Removes = int(index.shape[0] * compressionRate / 100.)\n",
    "        _, rindexes = pruneConvWithIndexes(e, index[num2Removes:])\n",
    "        pruneCBias(e, rindexes)\n",
    "        prevOutput = e.out_channels\n",
    "        prevTensor = e\n",
    "    if i == 28:\n",
    "        e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:32:09.907761Z",
     "start_time": "2018-04-30T16:32:09.889409Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in hooklist:\n",
    "    i.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T17:06:33.152431Z",
     "start_time": "2018-04-30T17:04:29.942268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 2/1293 0.043s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.029s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0036357371804685915"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T18:17:06.785928Z",
     "start_time": "2018-04-30T18:16:59.936753Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_70_eff.p')\n",
    "torch.save(fasterRCNN, save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T18:48:14.568193Z",
     "start_time": "2018-05-03T18:48:08.481407Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_70_eff.p')\n",
    "fasterRCNN = torch.load(save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T19:12:29.689214Z",
     "start_time": "2018-05-03T19:12:29.523683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (5): Conv2d(20, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(39, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (10): Conv2d(39, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(77, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(77, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (17): Conv2d(77, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace)\n",
       "  (19): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (24): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace)\n",
       "  (26): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace)\n",
       "  (28): Conv2d(154, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.RCNN_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T19:53:58.846502Z",
     "start_time": "2018-05-14T19:42:52.893139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_detect: 1/1293 34.649s 0.002s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.045s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0036357371804685915"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN = torch.load(\"pth_train_dir/vgg16/finished_models/pruned_70_eff.p\")\n",
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T18:48:57.128741Z",
     "start_time": "2018-05-03T18:48:46.433983Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pth_train_dir/vgg16/pruned_eff_70/vgg16/holly/faster_rcnn_1_1_200000.pth\")\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T18:52:36.739898Z",
     "start_time": "2018-05-03T18:49:25.621943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_detect: 2/1293 0.083s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.044s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7418562571885307"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
