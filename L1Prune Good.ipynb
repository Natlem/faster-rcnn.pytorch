{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:49.358061Z",
     "start_time": "2018-04-01T05:24:49.335278Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:54.769019Z",
     "start_time": "2018-04-01T05:24:49.844662Z"
    }
   },
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from scipy.misc import imread\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "from model.nms.nms_wrapper import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.utils.blob import im_list_to_blob\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:54.819307Z",
     "start_time": "2018-04-01T05:24:54.771900Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "    im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "    blob (ndarray): a data blob holding an image pyramid\n",
    "    im_scale_factors (list): list of image scales (relative to im) used\n",
    "      in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= cfg.PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in cfg.TEST.SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n",
    "            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:54.861273Z",
     "start_time": "2018-04-01T05:24:54.821791Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_from_file(\"cfgs/res101.yml\")\n",
    "set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "cfg_from_list(set_cfgs)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "load_model_path = os.path.join(\"models/res101/holly/good_models/faster_rcnn_1_1_117000.pth\")\n",
    "holly_classes = np.asarray(['__background__', 'head'])\n",
    "isCuda = True\n",
    "cfg.CUDA = isCuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:54.952890Z",
     "start_time": "2018-04-01T05:24:54.864375Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getDiff(old_tensor, new_tensor, seq, detail=False):\n",
    "    total_difference = 0\n",
    "    diff = 0\n",
    "    original = 0  \n",
    "    if \"Conv2d\" in str(old_tensor) and not hasattr(old_tensor, \"__getitem__\") :\n",
    "        old_shape = old_tensor.weight.shape[0]\n",
    "        new_shape = new_tensor.weight.shape[0]\n",
    "        diff = old_shape - new_shape\n",
    "        original += old_shape\n",
    "        total_difference += diff\n",
    "        if detail:\n",
    "            print(str(e_old) + \"Difference = {}\".format(diff))\n",
    "    elif \"Sequential\" in str(old_tensor) and hasattr(old_tensor, \"__getitem__\"):\n",
    "        for j,b in enumerate(old_tensor):\n",
    "            bn_old = vars(b)\n",
    "            bn_new = vars(new_tensor[j])\n",
    "            convs_old = bn_old[\"_modules\"]\n",
    "            convs_new = bn_new[\"_modules\"]\n",
    "            for k,v in convs_old.items():\n",
    "                if \"conv\" in k:\n",
    "                    old_shape = v.weight.shape[0]\n",
    "                    original += old_shape\n",
    "                    new_shape = convs_new[k].weight.shape[0]\n",
    "                    diff = old_shape - new_shape\n",
    "                    if detail:\n",
    "                        print(\"S {}/ B {}/ {} Difference = {}\".format(seq, j, k, diff))\n",
    "                    total_difference += diff\n",
    "    return original, total_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:54.982857Z",
     "start_time": "2018-04-01T05:24:54.954909Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_cuda(isCuda, tensor):\n",
    "    if isCuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:26:59.501856Z",
     "start_time": "2018-04-01T05:26:57.357601Z"
    },
    "code_folding": [
     0,
     3,
     12,
     25,
     37,
     47,
     59,
     69,
     79,
     88,
     94,
     115
    ]
   },
   "outputs": [],
   "source": [
    "def getNumpy(tensor):\n",
    "    return tensor.data.cpu().numpy()\n",
    "\n",
    "def sortFilters(filters):\n",
    "    #Sort filters by L1 Norm\n",
    "    c = filters.reshape(-1, filters.shape[0])\n",
    "    c = np.linalg.norm(c, ord=1, axis=0)\n",
    "    d = np.zeros([2, c.shape[0]])\n",
    "    i = np.argsort(c)\n",
    "    d[0,:] = i\n",
    "    d[1,:] = c[i]\n",
    "    return d\n",
    "def removeFilters(filters, percent, threshold=None):\n",
    "    #Remove filters from numpy \n",
    "    t = threshold\n",
    "    if threshold is None:\n",
    "        t = np.median(filters[1,:]) - np.std(filters[1,:])\n",
    "    mask = filters[1, :] > t\n",
    "    numRemoved_1 = mask.sum()\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - percent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def removePercent(filters, KeepPercent, PrunePercent):\n",
    "    #Remove filters from numpy \n",
    "    numOfRemove = filters.shape[1] * PrunePercent / 100.\n",
    "    numRemoved_1 = int(numOfRemove)\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - KeepPercent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    print(\"NumOfZeros:\")\n",
    "    print(numOfZeros)\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def sortRecoverFilters(l1Array, filters):\n",
    "    #Recover the order of the pruned filters\n",
    "    b = np.argsort(l1Array[0, :])\n",
    "    c = l1Array[:,b]\n",
    "    c = c[0,:].astype(int)\n",
    "    shape = np.asarray(filters.shape)\n",
    "    shape[0] = c.shape[0]\n",
    "    newFilters = np.zeros(shape)\n",
    "    newFilters = filters[c,:,:,:]\n",
    "    return newFilters\n",
    "def sortRecoverBatch(bn_tensor, index):\n",
    "    #Batch pruning \n",
    "    index.sort()\n",
    "    bn_rmean = bn_tensor.running_mean.cpu().numpy()\n",
    "    bn_tensor.running_mean = torch.from_numpy(bn_rmean[index]).float().cuda()\n",
    "    bn_rvar = bn_tensor.running_var.cpu().numpy()\n",
    "    bn_tensor.running_var = torch.from_numpy(bn_rvar[index]).float().cuda()\n",
    "    bn_weight = bn_tensor.weight.data.cpu().numpy()\n",
    "    bn_tensor.weight.data = torch.from_numpy(bn_weight[index]).float().cuda()\n",
    "    bn_bias = bn_tensor.bias.data.cpu().numpy()\n",
    "    bn_tensor.bias.data = torch.from_numpy(bn_bias[index]).float().cuda()\n",
    "\n",
    "def pruneConvLayers(tensor, percent = 10, threshold = None):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removeFilters(d, percent, threshold)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvPercent(tensor, KeepPercent = 10, PrunePercent=10):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removePercent(d, KeepPercent, PrunePercent)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvLowest(tensor, numKeep):\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = d[:,0:numKeep]\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvWithIndexes(tensor, rindexes):\n",
    "    #Prune out channels of convolutional layers using indexes\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    tensor.weight.data = torch.from_numpy(filters[rindexes]).float().cuda()\n",
    "    tensor.out_channels = tensor.weight.data.shape[0]\n",
    "    return tensor, rindexes\n",
    "def pruneNextLayer(nextLayerTensor, prevOutput, rindexes=None):\n",
    "    #Prune input channels of everything\n",
    "    if \"BatchNorm\" in str(nextLayerTensor):\n",
    "        nextLayerTensor.num_features = prevOutput\n",
    "        sortRecoverBatch(nextLayerTensor, rindexes)\n",
    "    elif \"Conv2d\" in str(nextLayerTensor):\n",
    "        if nextLayerTensor.weight.shape[1] == prevOutput:\n",
    "            return\n",
    "        nextLayerTensor.in_channels = prevOutput\n",
    "        nextConvWeight = getNumpy(nextLayerTensor.weight)\n",
    "        if not rindexes is None:\n",
    "            c = nextConvWeight[:,rindexes,:,:]\n",
    "        else:\n",
    "            c = nextConvWeight\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(c).float().cuda()\n",
    "    elif \"Linear\" in str(nextLayerTensor):\n",
    "        n = getNumpy(nextLayerTensor.weight)\n",
    "        fc = n[:,r]\n",
    "        nextLayerTensor.in_features = fc.shape[1]\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(fc).float().cuda()\n",
    "    return nextLayerTensor\n",
    "def prune_bottleneck(bottlenecks, prevOutput, rindexesX, keeplast=False):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, prevOutput, rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv1) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv2) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], prevOutput, rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvLayers(b_neck[0]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "        else:\n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "\n",
    "def prune_bottleneck_percent(bottlenecks, rindexesX, percents):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, len(rindexesX), rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv1, percents[0]) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv2, percents[1]) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], len(rindexesX), rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvPercent(b_neck[0], percents[2]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "        else:\n",
    "            #lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:24:59.102751Z",
     "start_time": "2018-04-01T05:24:59.017948Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pruneEverything():\n",
    "    percent = 10\n",
    "    prevIndex = 0\n",
    "    prevOutput = 3\n",
    "    rindexes = None\n",
    "    for i,e in enumerate(fasterRCNN.RCNN_base):\n",
    "        currentTensor = e\n",
    "        if \"Conv2d\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.in_channels:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput)\n",
    "            currentTensor, rindexes = pruneConvLayers(currentTensor, percent)\n",
    "            prevOutput = currentTensor.out_channels\n",
    "\n",
    "        elif \"BatchNorm\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.num_features:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput, rindexes)           \n",
    "            prevOutput = currentTensor.num_features\n",
    "        elif hasattr(currentTensor, \"__getitem__\"):\n",
    "            b, prevOutput, rindexes = prune_bottleneck(currentTensor, prevOutput, rindexes)\n",
    "\n",
    "        prevIndex = i\n",
    "        prevTensor = currentTensor\n",
    "\n",
    "    #Prune RoI pooling and FC classifiers\n",
    "    b = pruneNextLayer(fasterRCNN.RCNN_rpn.RPN_Conv, prevOutput, rindexes)\n",
    "    b, p, r = prune_bottleneck(fasterRCNN.RCNN_top[0], prevOutput, rindexes)\n",
    "    fc = pruneNextLayer(fasterRCNN.RCNN_bbox_pred, p, r)\n",
    "    cls_score = pruneNextLayer(fasterRCNN.RCNN_cls_score, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:14.733738Z",
     "start_time": "2018-04-01T06:00:05.860078Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_path = \"models/res101/holly/good_models/original_res101.pth\"\n",
    "fasterRCNN = resnet(holly_classes, 101, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()\n",
    "checkpoint = torch.load(load_model_path)\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:14.801056Z",
     "start_time": "2018-04-01T06:00:14.738595Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def reload_model(opruneModelPath, weightsPTH=None):\n",
    "    fasterRCNN = resnet(holly_classes, 101, pretrained=False, class_agnostic=False)\n",
    "    fasterRCNN = torch.load(opruneModelPath)\n",
    "    if not weightsPTH is None:\n",
    "        checkpoint = torch.load(weightsPTH)\n",
    "        fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "        if 'pooling_mode' in checkpoint.keys():\n",
    "            cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "    return fasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:14.961046Z",
     "start_time": "2018-04-01T06:00:14.817111Z"
    }
   },
   "outputs": [],
   "source": [
    "if isCuda:\n",
    "    fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:50:17.698587Z",
     "start_time": "2018-04-01T05:50:15.460273Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evalNetwork(modelFR, out=False):\n",
    "    modelFR.eval()\n",
    "    \n",
    "    cfg.TRAIN.USE_FLIPPED = False\n",
    "    imdb_name = \"holly_test\"\n",
    "    imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name, False)\n",
    "    imdb.competition_mode(on=True)\n",
    "    \n",
    "    save_name = 'faster_rcnn_10'\n",
    "    num_images = len(imdb.image_index)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "               for _ in range(imdb.num_classes)]\n",
    "\n",
    "    output_dir = get_output_dir(imdb, save_name)\n",
    "\n",
    "    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                            imdb.num_classes, training=False, normalize = False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=0,\n",
    "                                pin_memory=True)\n",
    "    output_dir = \"/tmp/outTest\"\n",
    "    vis = False\n",
    "    thresh = 0.0\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    num_images = len(imdb.image_index)\n",
    "\n",
    "    start = time.time()\n",
    "    max_per_image = 100\n",
    "\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        data = next(data_iter)\n",
    "        im_dataC.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_infoC.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxesC.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxesC.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "        rois_label = modelFR(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        if cfg.TEST.BBOX_REG:\n",
    "              # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "              # Optionally normalize targets by a precomputed mean and stdev\n",
    "                if False:#args.class_agnostic:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4)\n",
    "                else:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_infoC.data, 1)\n",
    "        else:\n",
    "              # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "        pred_boxes /= data[1][0][2]\n",
    "\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "        if vis:\n",
    "            im = cv2.imread(imdb.image_path_at(i))\n",
    "            im2show = np.copy(im)\n",
    "        for j in range(1, imdb.num_classes):\n",
    "            inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "              # if there is det\n",
    "            if inds.numel() > 0:\n",
    "                cls_scores = scores[:,j][inds]\n",
    "                _, order = torch.sort(cls_scores, 0, True)\n",
    "                if False:#args.class_agnostic:\n",
    "                    cls_boxes = pred_boxes[inds, :]\n",
    "                else:\n",
    "                    cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "                cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "                # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "                cls_dets = cls_dets[order]\n",
    "                keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "                cls_dets = cls_dets[keep.view(-1).long()]\n",
    "                if vis:\n",
    "                    im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "                all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "            else:\n",
    "                all_boxes[j][i] = empty_array\n",
    "\n",
    "          # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "            image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                        for j in range(1, imdb.num_classes)])\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "                for j in range(1, imdb.num_classes):\n",
    "                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "          .format(i + 1, num_images, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if vis:\n",
    "            cv2.imwrite('result.png', im2show)\n",
    "            pdb.set_trace()\n",
    "\n",
    "    with open(det_file, 'wb+') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    ap = imdb.evaluate_detections(all_boxes, output_dir, out)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:50:17.783980Z",
     "start_time": "2018-04-01T05:50:17.717240Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def prune(convTensor, nextBN, nextConv, percent, model, nextShortcutTensor = None, KeepPercent = 10):\n",
    "    convT = convTensor\n",
    "    o_channels = convT.out_channels\n",
    "    print(o_channels)\n",
    "    _, r = pruneConvPercent(convT, KeepPercent, percent)\n",
    "    print(\"PruneFiltersNum : {}\".format(o_channels - r.shape[0]))\n",
    "    pruneNextLayer(nextBN, r.shape[0], r)\n",
    "    pruneNextLayer(nextConv, r.shape[0], r)\n",
    "    pruneNextLayer(nextShortcutTensor, r.shape[0], r)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:15.034389Z",
     "start_time": "2018-04-01T06:00:14.975942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 6\n"
     ]
    }
   ],
   "source": [
    "r = prune(fasterRCNN.RCNN_base[0], fasterRCNN.RCNN_base[1], fasterRCNN.RCNN_base[4][0].conv1, 12, fasterRCNN, fasterRCNN.RCNN_base[4][0].downsample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T20:57:14.587105Z",
     "start_time": "2018-03-30T20:54:59.388442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__trainval` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__trainval gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__trainval_gt_roidb.pkl\n",
      "done\n",
      "before filtering, there are 222643 images...\n",
      "after filtering, there are 222643 images...\n"
     ]
    }
   ],
   "source": [
    "imdb_name = \"holly_trainval\"\n",
    "imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T20:58:32.265133Z",
     "start_time": "2018-03-30T20:58:32.206724Z"
    }
   },
   "outputs": [],
   "source": [
    "class sampler(Sampler):\n",
    "    def __init__(self, train_size, batch_size):\n",
    "        self.num_data = train_size\n",
    "        self.num_per_batch = int(train_size / batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
    "        self.leftover_flag = False\n",
    "        if train_size % batch_size:\n",
    "            self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "            self.leftover_flag = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
    "        self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
    "        self.rand_num_view = self.rand_num.view(-1)\n",
    "\n",
    "        if self.leftover_flag:\n",
    "            self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
    "\n",
    "        return iter(self.rand_num_view)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T20:58:34.200Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fineTuneModel(model, imdb):\n",
    "    batch_size = 2\n",
    "    cfg.TRAIN.USE_FLIPPED = True\n",
    "    cfg.USE_GPU_NMS = True\n",
    "    start_epoch = 0\n",
    "    max_epochs = 1\n",
    "    lr_decay_step = 5\n",
    "    lr_decay_gamma = 0.1\n",
    "    disp_interval = 100\n",
    "    output_dir = \"./pth_train_dir/\"\n",
    "    class_agnostic = False\n",
    "    session = 1\n",
    "    train_size = len(roidb)\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    sampler_batch = sampler(train_size, batch_size)\n",
    "    dataset_train = roibatchLoader(roidb, ratio_list, ratio_index, batch_size, imdb.num_classes, training=True)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=sampler_batch, num_workers=1)\n",
    "    im_dataT = torch.FloatTensor(1)\n",
    "    im_infoT = torch.FloatTensor(1)\n",
    "    num_boxesT = torch.LongTensor(1)\n",
    "    gt_boxesT = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataT = make_cuda(isCuda, im_dataT)\n",
    "    im_infoT = make_cuda(isCuda, im_infoT)\n",
    "    num_boxesT = make_cuda(isCuda, num_boxesT)\n",
    "    gt_boxesT = make_cuda(isCuda, gt_boxesT)\n",
    "\n",
    "      # make variable\n",
    "    im_dataT = Variable(im_dataT)\n",
    "    im_infoT = Variable(im_infoT)\n",
    "    num_boxesT = Variable(num_boxesT)\n",
    "    gt_boxesT = Variable(gt_boxesT)\n",
    "\n",
    "    lr = cfg.TRAIN.LEARNING_RATE\n",
    "    params = []\n",
    "    for key, value in dict(fasterRCNN.named_parameters()).items():\n",
    "        if value.requires_grad:\n",
    "            if 'bias' in key:\n",
    "                params += [{'params':[value],'lr':lr*(cfg.TRAIN.DOUBLE_BIAS + 1), \\\n",
    "                            'weight_decay': cfg.TRAIN.BIAS_DECAY and cfg.TRAIN.WEIGHT_DECAY or 0}]\n",
    "            else:\n",
    "                params += [{'params':[value],'lr':lr, 'weight_decay': cfg.TRAIN.WEIGHT_DECAY}]\n",
    "\n",
    "    optimizer = torch.optim.SGD(params, momentum=cfg.TRAIN.MOMENTUM)\n",
    "    iters_per_epoch = int(train_size / batch_size)\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs + 1):\n",
    "        # setting to train mode\n",
    "        fasterRCNN.train()\n",
    "        loss_temp = 0\n",
    "        start = time.time()\n",
    "\n",
    "        if epoch % (lr_decay_step + 1) == 0:\n",
    "            adjust_learning_rate(optimizer, lr_decay_gamma)\n",
    "            lr *= lr_decay_gamma\n",
    "\n",
    "        data_iter = iter(dataloader_train)\n",
    "        for step in range(iters_per_epoch):\n",
    "            data = next(data_iter)\n",
    "            im_dataT.data.resize_(data[0].size()).copy_(data[0])\n",
    "            im_infoT.data.resize_(data[1].size()).copy_(data[1])\n",
    "            gt_boxesT.data.resize_(data[2].size()).copy_(data[2])\n",
    "            num_boxesT.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "            fasterRCNN.zero_grad()\n",
    "            rois, cls_prob, bbox_pred, \\\n",
    "            rpn_loss_cls, rpn_loss_box, \\\n",
    "            RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "            rois_label = fasterRCNN(im_dataT, im_infoT, gt_boxesT, num_boxesT)\n",
    "\n",
    "            loss = rpn_loss_cls.mean() + rpn_loss_box.mean() \\\n",
    "               + RCNN_loss_cls.mean() + RCNN_loss_bbox.mean()\n",
    "            loss_temp += loss.data[0]\n",
    "\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % disp_interval == 0:\n",
    "                save_name = os.path.join(output_dir, 'faster_rcnn_{}_{}_{}.pth'.format(session, epoch, step))\n",
    "                save_checkpoint({\n",
    "                    'session': session,\n",
    "                    'epoch': epoch + 1,\n",
    "                    'model': fasterRCNN.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'pooling_mode': cfg.POOLING_MODE,\n",
    "                    'class_agnostic': class_agnostic,\n",
    "                    }, save_name)\n",
    "\n",
    "                end = time.time()\n",
    "                if step > 0:\n",
    "                    loss_temp /= disp_interval\n",
    "\n",
    "                loss_rpn_cls = rpn_loss_cls.data[0]\n",
    "                loss_rpn_box = rpn_loss_box.data[0]\n",
    "                loss_rcnn_cls = RCNN_loss_cls.data[0]\n",
    "                loss_rcnn_box = RCNN_loss_bbox.data[0]\n",
    "                fg_cnt = torch.sum(rois_label.data.ne(0))\n",
    "                bg_cnt = rois_label.data.numel() - fg_cnt\n",
    "\n",
    "                print(\"[session %d][epoch %2d][iter %4d/%4d] loss: %.4f, lr: %.2e\" \\\n",
    "                                        % (session, epoch, step, iters_per_epoch, loss_temp, lr))\n",
    "                print(\"\\t\\t\\tfg/bg=(%d/%d), time cost: %f\" % (fg_cnt, bg_cnt, end-start))\n",
    "                print(\"\\t\\t\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box %.4f\" \\\n",
    "                              % (loss_rpn_cls, loss_rpn_box, loss_rcnn_cls, loss_rcnn_box))\n",
    "\n",
    "                loss_temp = 0\n",
    "                start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-30T20:58:34.606Z"
    }
   },
   "outputs": [],
   "source": [
    "fineTuneModel(fasterRCNN, imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:15.819472Z",
     "start_time": "2018-04-01T06:00:15.051607Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pth_train_dir/faster_rcnn_1_0_111300.pth\")\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T05:43:18.728620Z",
     "start_time": "2018-04-01T05:38:05.346781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 1/1293 0.177s 0.002s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.156s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7787543291411585"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:27.103804Z",
     "start_time": "2018-04-01T06:00:26.976650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n",
      "NumOfZeros:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "_,no, r = prune_bottleneck_percent(fasterRCNN.RCNN_base[4], r, [0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:00:29.388609Z",
     "start_time": "2018-04-01T06:00:29.330587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(231, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruneNextLayer(fasterRCNN.RCNN_base[5][0].conv1, len(r), r)\n",
    "pruneNextLayer(fasterRCNN.RCNN_base[5][0].downsample[0], len(r), r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:05:38.421424Z",
     "start_time": "2018-04-01T06:00:36.124223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 1/1293 0.206s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.208s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    }
   ],
   "source": [
    "ap = evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T06:07:53.894890Z",
     "start_time": "2018-04-01T06:07:53.736321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013666499637356947"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T17:52:46.153446Z",
     "start_time": "2018-03-30T17:52:39.622049Z"
    }
   },
   "outputs": [],
   "source": [
    "test = reload_model(save_name_model, weightsPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T17:53:11.777725Z",
     "start_time": "2018-03-30T17:53:11.730445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 58, 1, 1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.RCNN_base[4][0].conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T18:01:23.081385Z",
     "start_time": "2018-03-30T17:58:08.323148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 60\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 1/1293 0.120s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.155s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    }
   ],
   "source": [
    "b_1_1_conv1 = test.RCNN_base[4][0].conv1\n",
    "o_channels = b_1_1_conv2.out_channels\n",
    "print(o_channels)\n",
    "b_1_1_conv2 = test.RCNN_base[4][0].conv2\n",
    "b_1_1_bn1 = test.RCNN_base[4][0].bn1\n",
    "_, r = pruneConvPercent(b_1_1_conv1, KeepPercent, 70)\n",
    "print(\"PruneFiltersNum : {}\".format(o_channels - r.shape[0]))\n",
    "pruneNextLayer(b_1_1_bn1, r.shape[0], r)\n",
    "pruneNextLayer(b_1_1_conv2, r.shape[0], r)\n",
    "ap = evalNetwork(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T18:01:50.138098Z",
     "start_time": "2018-03-30T18:01:49.974770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777952685259206"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T13:47:32.210421Z",
     "start_time": "2018-03-27T13:47:32.171348Z"
    }
   },
   "outputs": [],
   "source": [
    "def prune1stConv(fasterRCNN):\n",
    "    PrunePercent = 12\n",
    "    KeepPercent = 10\n",
    "    conv1st = fasterRCNN.RCNN_base[0]\n",
    "    bn1 = fasterRCNN.RCNN_base[1]\n",
    "    b_1_1_conv1 = fasterRCNN.RCNN_base[4][0].conv1\n",
    "    b_1_1_shortcut = fasterRCNN.RCNN_base[4][0].downsample[0]\n",
    "    _ , r = pruneConvPercent(conv1st, KeepPercent, PrunePercent)\n",
    "    pruneNextLayer(bn1, r.shape[0], r)\n",
    "    p_1_1_c1 = pruneNextLayer(b_1_1_conv1, r.shape[0], r)\n",
    "    p_1_1_shortcut = pruneNextLayer(b_1_1_shortcut, r.shape[0], r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T13:47:38.291212Z",
     "start_time": "2018-03-27T13:47:32.730388Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrunePercent : 98\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 6\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-41af7e57bb75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mpruneNextLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_1_1_bn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpruneNextLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_1_1_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasterRCNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AP: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalAcc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a551f910b269>\u001b[0m in \u001b[0;36mevalNetwork\u001b[0;34m(modelFR, out)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSE_FLIPPED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"holly_test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mimdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroidb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_roidb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompetition_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36mcombined_roidb\u001b[0;34m(imdb_names, training)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroidb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0mroidbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_roidb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimdb_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m   \u001b[0mroidb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroidbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroidb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0mroidbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_roidb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimdb_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m   \u001b[0mroidb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroidbs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36mget_roidb\u001b[0;34m(imdb_name)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_proposal_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROPOSAL_METHOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Set proposal method: {:s}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROPOSAL_METHOD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mroidb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_training_roidb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mroidb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36mget_training_roidb\u001b[0;34m(imdb)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing training data...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mprepare_roidb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;31m#ratio_index = rank_roidb_ratio(imdb)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36mprepare_roidb\u001b[0;34m(imdb)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'coco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n\u001b[0;32m---> 24\u001b[0;31m          for i in range(imdb.num_images)]\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/roi_data_layer/roidb.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'coco'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n\u001b[0;32m---> 24\u001b[0;31m          for i in range(imdb.num_images)]\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2552\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Pruning of first conv1 of block 1, bottleneck 1\n",
    "\n",
    "weightsPTH = \"models/res101/holly/faster_rcnn_1_1_52000.pth\"\n",
    "prunePercent = range(98, 100, 1)\n",
    "originalAcc = 0.758\n",
    "KeepPercent = 10\n",
    "for i in prunePercent:\n",
    "    print(\"PrunePercent : {}\".format(i))\n",
    "    fasterRCNN = reload_model(save_name_model, weightsPTH)\n",
    "    b_1_1_conv1 = fasterRCNN.RCNN_base[4][0].conv1\n",
    "    o_channels = b_1_1_conv1.out_channels\n",
    "    b_1_1_conv2 = fasterRCNN.RCNN_base[4][0].conv2\n",
    "    b_1_1_bn1 = fasterRCNN.RCNN_base[4][0].bn1\n",
    "    _, r = pruneConvPercent(b_1_1_conv1, KeepPercent, i)\n",
    "    print(\"PruneFiltersNum : {}\".format(o_channels - r.shape[0]))\n",
    "    pruneNextLayer(b_1_1_bn1, r.shape[0], r)\n",
    "    pruneNextLayer(b_1_1_conv2, r.shape[0], r)\n",
    "    ap = evalNetwork(fasterRCNN)\n",
    "    print(\"AP: {}\".format(ap))\n",
    "    if np.abs(originalAcc - ap) > 0.4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-27T13:47:40.269Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrunePercent : 10\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 5\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 1/1293 0.110s 0.002s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n",
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.086s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7220566953398896\n",
      "PrunePercent : 12\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 6\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.109s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6984362425509368\n",
      "PrunePercent : 14\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 7\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.115s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6984362425509368\n",
      "PrunePercent : 16\n",
      "NumOfZeros:\n",
      "1\n",
      "PruneFiltersNum : 9\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.074s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7570614008098199\n",
      "PrunePercent : 18\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 9\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.085s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7570614008098199\n",
      "PrunePercent : 20\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 10\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.074s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.757270119987296\n",
      "PrunePercent : 22\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 12\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.098s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.757270119987296\n",
      "PrunePercent : 24\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 13\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.149s 0.005s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7584099270452405\n",
      "PrunePercent : 26\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 14\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.115s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7398228375732487\n",
      "PrunePercent : 28\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 15\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.144s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7673944576278666\n",
      "PrunePercent : 30\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 17\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.074s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.7718258452707755\n",
      "PrunePercent : 32\n",
      "NumOfZeros:\n",
      "2\n",
      "PruneFiltersNum : 18\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.107s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6908325159239929\n",
      "PrunePercent : 34\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 18\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.087s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6908325159239929\n",
      "PrunePercent : 36\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 20\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.142s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6240483333214977\n",
      "PrunePercent : 38\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 21\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.101s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6240483333214977\n",
      "PrunePercent : 40\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 22\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.106s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6240483333214977\n",
      "PrunePercent : 42\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 23\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.097s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6240483333214977\n",
      "PrunePercent : 44\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 25\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.146s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.6129829501582982\n",
      "PrunePercent : 46\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 26\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.107s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.5847208192334201\n",
      "PrunePercent : 48\n",
      "NumOfZeros:\n",
      "3\n",
      "PruneFiltersNum : 27\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.074s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.5462001279200344\n",
      "PrunePercent : 50\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 28\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.099s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.5370614859889757\n",
      "PrunePercent : 52\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 29\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.089s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.5370614859889757\n",
      "PrunePercent : 54\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 30\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.077s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.49292562491960124\n",
      "PrunePercent : 56\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 31\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Evaluating detections0.123s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.502514688747505\n",
      "PrunePercent : 58\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 33\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.104s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.49872285141737127\n",
      "PrunePercent : 60\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 34\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.130s 0.002s   \n",
      "Writing head VOC results file\n",
      "AP: 0.49872285141737127\n",
      "PrunePercent : 62\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 35\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.093s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.521498350007713\n",
      "PrunePercent : 64\n",
      "NumOfZeros:\n",
      "4\n",
      "PruneFiltersNum : 36\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.112s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.521498350007713\n",
      "PrunePercent : 66\n",
      "NumOfZeros:\n",
      "5\n",
      "PruneFiltersNum : 37\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.117s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.521498350007713\n",
      "PrunePercent : 68\n",
      "NumOfZeros:\n",
      "5\n",
      "PruneFiltersNum : 38\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "Evaluating detections0.135s 0.001s   \n",
      "Writing head VOC results file\n",
      "AP: 0.5500327213789347\n",
      "PrunePercent : 70\n",
      "NumOfZeros:\n",
      "5\n",
      "PruneFiltersNum : 39\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n",
      "im_detect: 678/1293 0.176s 0.001s   \r"
     ]
    }
   ],
   "source": [
    "#Pruning of first conv2 of block 1, bottleneck 1\n",
    "weightsPTH = \"models/res101/holly/faster_rcnn_1_1_52000.pth\"\n",
    "save_name_model = os.path.join(\"./models/res101/holly/\", 'pruned_117000_2.p')\n",
    "prunePercent = range(10, 100, 2)\n",
    "originalAcc = 0.758\n",
    "KeepPercent = 10\n",
    "for i in prunePercent:\n",
    "    print(\"PrunePercent : {}\".format(i))\n",
    "    fasterRCNN = reload_model(save_name_model, weightsPTH)\n",
    "    b_1_1_conv2 = fasterRCNN.RCNN_base[4][0].conv2\n",
    "    o_channels = b_1_1_conv2.out_channels\n",
    "    b_1_1_conv3 = fasterRCNN.RCNN_base[4][0].conv3\n",
    "    b_1_1_bn2 = fasterRCNN.RCNN_base[4][0].bn2\n",
    "    _, r = pruneConvPercent(b_1_1_conv2, KeepPercent, i)\n",
    "    print(\"PruneFiltersNum : {}\".format(o_channels - r.shape[0]))\n",
    "    pruneNextLayer(b_1_1_bn2, r.shape[0], r)\n",
    "    pruneNextLayer(b_1_1_conv3, r.shape[0], r)\n",
    "    ap = evalNetwork(fasterRCNN)\n",
    "    print(\"AP: {}\".format(ap))\n",
    "    if np.abs(originalAcc - ap) > 0.4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T18:15:32.326602Z",
     "start_time": "2018-03-30T18:15:32.150983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 58, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(58, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(9, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(58, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.RCNN_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T18:20:11.387673Z",
     "start_time": "2018-03-30T18:20:09.729189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrunePercent : 10\n",
      "NumOfZeros:\n",
      "0\n",
      "PruneFiltersNum : 0\n",
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 256 elements not 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-74247ddf4287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpruneNextLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_1_1_bn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpruneNextLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_1_1_conv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevalNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfasterRCNN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AP: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginalAcc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a551f910b269>\u001b[0m in \u001b[0;36mevalNetwork\u001b[0;34m(modelFR, out)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mdet_tic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mrpn_loss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrpn_loss_box\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mRCNN_loss_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRCNN_loss_bbox\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mrois_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_dataC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_infoC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_boxesC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boxesC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, im_data, im_info, gt_boxes, num_boxes)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# feed image data to base model to obtain base feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mbase_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRCNN_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# feed base feature map tp RPN to obtain rois\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         return F.batch_norm(\n\u001b[1;32m     36\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             self.training, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3ml/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected more than 1 value per channel when training, got input size {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 256 elements not 4"
     ]
    }
   ],
   "source": [
    "#Prune \n",
    "weightsPTH = \"models/res101/holly/faster_rcnn_1_1_52000.pth\"\n",
    "save_name_model = os.path.join(\"./models/res101/holly/\", 'pruned_117000_2.p')\n",
    "prunePercent = range(10, 100, 10)\n",
    "originalAcc = 0.55\n",
    "KeepPercent = 10\n",
    "for i in prunePercent:\n",
    "    print(\"PrunePercent : {}\".format(i))\n",
    "    fasterRCNN = reload_model(save_name_model, weightsPTH)\n",
    "    b_1_1_shortcut = fasterRCNN.RCNN_base[4][0].downsample[0]\n",
    "    o_channels = b_1_1_conv1.out_channels\n",
    "    b_1_2_conv1 = fasterRCNN.RCNN_base[4][1].conv1\n",
    "    b_1_1_bn1 = fasterRCNN.RCNN_base[4][0].downsample[1]\n",
    "    _, r = pruneConvPercent(b_1_1_conv1, KeepPercent, i)\n",
    "    print(\"PruneFiltersNum : {}\".format(o_channels - r.shape[0]))\n",
    "    pruneNextLayer(b_1_1_bn1, r.shape[0], r)\n",
    "    pruneNextLayer(b_1_1_conv2, r.shape[0], r)\n",
    "    ap = evalNetwork(fasterRCNN)\n",
    "    print(\"AP: {}\".format(ap))\n",
    "    if np.abs(originalAcc - ap) > 0.4:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T17:43:59.935342Z",
     "start_time": "2018-03-30T17:43:59.780998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(9, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_1_1_conv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-30T17:43:36.808532Z",
     "start_time": "2018-03-30T17:43:36.779387Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.RCNN_base[4][0].conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T04:32:37.437197Z",
     "start_time": "2018-03-27T04:32:33.056Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_path = \"models/res101/holly/faster_rcnn_1_1_52000.pth\"\n",
    "save_name_model = os.path.join(\"./models/res101/holly/\", 'pruned_117000_2.p')\n",
    "fasterRCNN = reload_model(save_name_model, load_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T04:10:05.891967Z",
     "start_time": "2018-03-27T04:10:04.866735Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def evalNetwork(modelFR, out=False):\n",
    "    modelFR.eval()\n",
    "    \n",
    "    cfg.TRAIN.USE_FLIPPED = False\n",
    "    imdb_name = \"holly_test\"\n",
    "    imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name, False)\n",
    "    imdb.competition_mode(on=True)\n",
    "    \n",
    "    save_name = 'faster_rcnn_10'\n",
    "    num_images = len(imdb.image_index)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "               for _ in range(imdb.num_classes)]\n",
    "\n",
    "    output_dir = get_output_dir(imdb, save_name)\n",
    "\n",
    "    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                            imdb.num_classes, training=False, normalize = False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=0,\n",
    "                                pin_memory=True)\n",
    "    output_dir = \"/tmp/outTest\"\n",
    "    vis = False\n",
    "    thresh = 0.0\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    num_images = len(imdb.image_index)\n",
    "\n",
    "    start = time.time()\n",
    "    max_per_image = 100\n",
    "\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        data = next(data_iter)\n",
    "        im_dataC.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_infoC.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxesC.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxesC.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "        rois_label = modelFR(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        if cfg.TEST.BBOX_REG:\n",
    "              # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "              # Optionally normalize targets by a precomputed mean and stdev\n",
    "                if False:#args.class_agnostic:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4)\n",
    "                else:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_infoC.data, 1)\n",
    "        else:\n",
    "              # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "        pred_boxes /= data[1][0][2]\n",
    "\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "        if vis:\n",
    "            im = cv2.imread(imdb.image_path_at(i))\n",
    "            im2show = np.copy(im)\n",
    "        for j in range(1, imdb.num_classes):\n",
    "            inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "              # if there is det\n",
    "            if inds.numel() > 0:\n",
    "                cls_scores = scores[:,j][inds]\n",
    "                _, order = torch.sort(cls_scores, 0, True)\n",
    "                if False:#args.class_agnostic:\n",
    "                    cls_boxes = pred_boxes[inds, :]\n",
    "                else:\n",
    "                    cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "                cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "                # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "                cls_dets = cls_dets[order]\n",
    "                keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "                cls_dets = cls_dets[keep.view(-1).long()]\n",
    "                if vis:\n",
    "                    im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "                all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "            else:\n",
    "                all_boxes[j][i] = empty_array\n",
    "\n",
    "          # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "            image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                        for j in range(1, imdb.num_classes)])\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "                for j in range(1, imdb.num_classes):\n",
    "                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "          .format(i + 1, num_images, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if vis:\n",
    "            cv2.imwrite('result.png', im2show)\n",
    "            pdb.set_trace()\n",
    "\n",
    "    with open(det_file, 'wb+') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    ap = imdb.evaluate_detections(all_boxes, output_dir, out)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T04:13:36.465228Z",
     "start_time": "2018-03-27T04:10:05.895284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_detect: 2/1293 0.108s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.093s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7901467630563271"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T21:53:34.026638Z",
     "start_time": "2018-03-26T21:53:31.819134Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./models/res101/holly/\", 'pruned_117000_2.p')\n",
    "torch.save(fasterRCNN, save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
