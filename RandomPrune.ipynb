{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:43.511695Z",
     "start_time": "2018-05-14T20:01:43.460166Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:57.500388Z",
     "start_time": "2018-05-14T20:01:43.513077Z"
    }
   },
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "from scipy.misc import imread\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "from model.nms.nms_wrapper import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.utils.blob import im_list_to_blob\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "from model.utils.net_utils import weights_normal_init, save_net, load_net, \\\n",
    "      adjust_learning_rate, save_checkpoint, clip_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:57.562046Z",
     "start_time": "2018-05-14T20:01:57.502425Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "    im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "    blob (ndarray): a data blob holding an image pyramid\n",
    "    im_scale_factors (list): list of image scales (relative to im) used\n",
    "      in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= cfg.PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in cfg.TEST.SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n",
    "            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:57.659044Z",
     "start_time": "2018-05-14T20:01:57.563671Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_from_file(\"cfgs/vgg16.yml\")\n",
    "set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "cfg_from_list(set_cfgs)\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "load_model_path = os.path.join(\"models/res101/holly/good_models/faster_rcnn_1_1_117000.pth\")\n",
    "holly_classes = np.asarray(['__background__', 'head'])\n",
    "isCuda = True\n",
    "cfg.CUDA = isCuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:57.734714Z",
     "start_time": "2018-05-14T20:01:57.661887Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def getDiff(old_tensor, new_tensor, seq, detail=False):\n",
    "    total_difference = 0\n",
    "    diff = 0\n",
    "    original = 0  \n",
    "    if \"Conv2d\" in str(old_tensor) and not hasattr(old_tensor, \"__getitem__\") :\n",
    "        old_shape = old_tensor.weight.shape[0]\n",
    "        new_shape = new_tensor.weight.shape[0]\n",
    "        diff = old_shape - new_shape\n",
    "        original += old_shape\n",
    "        total_difference += diff\n",
    "        if detail:\n",
    "            print(str(e_old) + \"Difference = {}\".format(diff))\n",
    "    elif \"Sequential\" in str(old_tensor) and hasattr(old_tensor, \"__getitem__\"):\n",
    "        for j,b in enumerate(old_tensor):\n",
    "            bn_old = vars(b)\n",
    "            bn_new = vars(new_tensor[j])\n",
    "            convs_old = bn_old[\"_modules\"]\n",
    "            convs_new = bn_new[\"_modules\"]\n",
    "            for k,v in convs_old.items():\n",
    "                if \"conv\" in k:\n",
    "                    old_shape = v.weight.shape[0]\n",
    "                    original += old_shape\n",
    "                    new_shape = convs_new[k].weight.shape[0]\n",
    "                    diff = old_shape - new_shape\n",
    "                    if detail:\n",
    "                        print(\"S {}/ B {}/ {} Difference = {}\".format(seq, j, k, diff))\n",
    "                    total_difference += diff\n",
    "    return original, total_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:57.779262Z",
     "start_time": "2018-05-14T20:01:57.736313Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_cuda(isCuda, tensor):\n",
    "    if isCuda:\n",
    "        return tensor.cuda()\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:59.322807Z",
     "start_time": "2018-05-14T20:01:57.781196Z"
    },
    "code_folding": [
     0,
     3,
     12,
     25,
     37,
     47,
     59,
     69,
     79,
     88,
     94,
     115,
     151,
     181
    ]
   },
   "outputs": [],
   "source": [
    "def getNumpy(tensor):\n",
    "    return tensor.data.cpu().numpy()\n",
    "\n",
    "def sortFilters(filters):\n",
    "    #Sort filters by L1 Norm\n",
    "    c = filters.reshape(-1, filters.shape[0])\n",
    "    c = np.linalg.norm(c, ord=1, axis=0)\n",
    "    d = np.zeros([2, c.shape[0]])\n",
    "    i = np.argsort(c)\n",
    "    d[0,:] = i\n",
    "    d[1,:] = c[i]\n",
    "    return d\n",
    "def removeFilters(filters, percent, threshold=None):\n",
    "    #Remove filters from numpy \n",
    "    t = threshold\n",
    "    if threshold is None:\n",
    "        t = np.median(filters[1,:]) - np.std(filters[1,:])\n",
    "    mask = filters[1, :] > t\n",
    "    numRemoved_1 = mask.sum()\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - percent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def removePercent(filters, KeepPercent, PrunePercent):\n",
    "    #Remove filters from numpy \n",
    "    numOfRemove = filters.shape[1] * PrunePercent / 100.\n",
    "    numRemoved_1 = int(numOfRemove)\n",
    "    numRemoved_2 = int(numRemoved_1 * ((100. - KeepPercent) / 100.))\n",
    "    numOfZeros = numRemoved_1 - numRemoved_2\n",
    "    newFilters = filters[:,numRemoved_2:].copy()\n",
    "    print(\"NumOfZeros:\")\n",
    "    print(numOfZeros)\n",
    "    newFilters[1, 0:numOfZeros] =0\n",
    "    return newFilters\n",
    "\n",
    "def sortRecoverFilters(l1Array, filters):\n",
    "    #Recover the order of the pruned filters\n",
    "    b = np.argsort(l1Array[0, :])\n",
    "    c = l1Array[:,b]\n",
    "    c = c[0,:].astype(int)\n",
    "    shape = np.asarray(filters.shape)\n",
    "    shape[0] = c.shape[0]\n",
    "    newFilters = np.zeros(shape)\n",
    "    newFilters = filters[c,:,:,:]\n",
    "    return newFilters\n",
    "def sortRecoverBatch(bn_tensor, index):\n",
    "    #Batch pruning \n",
    "    index.sort()\n",
    "    bn_rmean = bn_tensor.running_mean.cpu().numpy()\n",
    "    bn_tensor.running_mean = torch.from_numpy(bn_rmean[index]).float().cuda()\n",
    "    bn_rvar = bn_tensor.running_var.cpu().numpy()\n",
    "    bn_tensor.running_var = torch.from_numpy(bn_rvar[index]).float().cuda()\n",
    "    bn_weight = bn_tensor.weight.data.cpu().numpy()\n",
    "    bn_tensor.weight.data = torch.from_numpy(bn_weight[index]).float().cuda()\n",
    "    bn_bias = bn_tensor.bias.data.cpu().numpy()\n",
    "    bn_tensor.bias.data = torch.from_numpy(bn_bias[index]).float().cuda()\n",
    "\n",
    "def pruneConvLayers(tensor, percent = 10, threshold = None):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removeFilters(d, percent, threshold)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvPercent(tensor, KeepPercent = 10, PrunePercent=10):\n",
    "    #Prune out channels of convolutional layers\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = removePercent(d, KeepPercent, PrunePercent)\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvLowest(tensor, numKeep):\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    d = sortFilters(filters)\n",
    "    e = d[:,0:numKeep]\n",
    "    f = sortRecoverFilters(e, filters)\n",
    "    tensor.weight.data = torch.from_numpy(f).float().cuda()\n",
    "    tensor.out_channels = f.shape[0]\n",
    "    rindexes = e[0,:].astype(int).copy()\n",
    "    return tensor, rindexes\n",
    "def pruneConvWithIndexes(tensor, rindexes):\n",
    "    #Prune out channels of convolutional layers using indexes\n",
    "    filters = getNumpy(tensor.weight)\n",
    "    tensor.weight.data = torch.from_numpy(filters[rindexes]).float().cuda()\n",
    "    tensor.out_channels = tensor.weight.data.shape[0]\n",
    "    return tensor, rindexes\n",
    "def pruneNextLayer(nextLayerTensor, prevOutput, rindexes=None):\n",
    "    #Prune input channels of everything\n",
    "    if \"BatchNorm\" in str(nextLayerTensor):\n",
    "        nextLayerTensor.num_features = prevOutput\n",
    "        sortRecoverBatch(nextLayerTensor, rindexes)\n",
    "    elif \"Conv2d\" in str(nextLayerTensor):\n",
    "        if nextLayerTensor.weight.shape[1] == prevOutput:\n",
    "            return\n",
    "        nextLayerTensor.in_channels = prevOutput\n",
    "        nextConvWeight = getNumpy(nextLayerTensor.weight)\n",
    "        if not rindexes is None:\n",
    "            c = nextConvWeight[:,rindexes,:,:]\n",
    "        else:\n",
    "            c = nextConvWeight\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(c).float().cuda()\n",
    "    elif \"Linear\" in str(nextLayerTensor):\n",
    "        n = getNumpy(nextLayerTensor.weight)\n",
    "        fc = n[:,rindexes]\n",
    "        nextLayerTensor.in_features = fc.shape[1]\n",
    "        nextLayerTensor.weight.data = torch.from_numpy(fc).float().cuda()\n",
    "    return nextLayerTensor\n",
    "def prune_bottleneck(bottlenecks, prevOutput, rindexesX, keeplast=False):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, prevOutput, rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv1) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvLayers(bottleneck.conv2) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], prevOutput, rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvLayers(b_neck[0]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "        else:\n",
    "            if not keeplast:\n",
    "                lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            else:\n",
    "                lastconv = bottleneck.conv3\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "\n",
    "def prune_bottleneck_percent(bottlenecks, rindexesX, percents):\n",
    "    for i,bottleneck in enumerate(bottlenecks):\n",
    "        #Conv1 Prune\n",
    "        currentTensor = pruneNextLayer(bottleneck.conv1, len(rindexesX), rindexesX) #Conv1 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv1, percents[0]) #Conv1 Output channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn1, currentTensor.out_channels, rindexes) #BN1 prune\n",
    "        \n",
    "        #Conv2 Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv2, currentTensor.num_features, rindexes) #Conv2 Input channels Prune\n",
    "        currentTensor, rindexes = pruneConvPercent(bottleneck.conv2, percents[1]) #Conv2 Input channels Prune\n",
    "        currentTensor           = pruneNextLayer(bottleneck.bn2, currentTensor.out_channels, rindexes) #BN2 prune\n",
    "        #Shortcut + conv2\n",
    "        currentTensor           = pruneNextLayer(bottleneck.conv3, currentTensor.num_features, rindexes) #Conv3 input channels\n",
    "        \n",
    "        #Prune shortcut first\n",
    "        if not bottleneck.downsample is None:\n",
    "            b_neck = bottleneck.downsample\n",
    "            pruneNextLayer(b_neck[0], len(rindexesX), rindexesX) #Shortcut Input Channels\n",
    "            shorcutconv, rindexes = pruneConvPercent(b_neck[0], percents[2]) #Shortcut output channels\n",
    "            pruneNextLayer(b_neck[1], b_neck[0].out_channels, rindexes) #bn1 of shorcut\n",
    "        \n",
    "            lastconv, rindexes = pruneConvWithIndexes(bottleneck.conv3, rindexes) #Conv3 output channels\n",
    "        else:\n",
    "            lastconv, rindexes = pruneConvLowest(bottleneck.conv3, len(rindexesX))\n",
    "            \n",
    "        lastbn = pruneNextLayer(bottleneck.bn3, currentTensor.out_channels, rindexes) #BN3 prune\n",
    "        num_output = lastconv.out_channels\n",
    "        rindexesX = rindexes\n",
    "    return bottleneck, num_output, rindexes\n",
    "\n",
    "def pruneCBias(convTensor, rindexes):\n",
    "    convT = convTensor\n",
    "    bias_data = convT.bias.data.cpu().numpy()\n",
    "    convT.bias.data = torch.from_numpy(bias_data[rindexes]).float().cuda()\n",
    "    return convT, rindexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:01:59.397226Z",
     "start_time": "2018-05-14T20:01:59.324577Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def pruneEverything():\n",
    "    percent = 10\n",
    "    prevIndex = 0\n",
    "    prevOutput = 3\n",
    "    rindexes = None\n",
    "    for i,e in enumerate(fasterRCNN.RCNN_base):\n",
    "        currentTensor = e\n",
    "        if \"Conv2d\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.in_channels:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput)\n",
    "            currentTensor, rindexes = pruneConvLayers(currentTensor, percent)\n",
    "            prevOutput = currentTensor.out_channels\n",
    "\n",
    "        elif \"BatchNorm\" in str(currentTensor) and not hasattr(currentTensor, \"__getitem__\"):\n",
    "            if prevOutput != currentTensor.num_features:\n",
    "                currentTensor = pruneNextLayer(currentTensor, prevOutput, rindexes)           \n",
    "            prevOutput = currentTensor.num_features\n",
    "        elif hasattr(currentTensor, \"__getitem__\"):\n",
    "            b, prevOutput, rindexes = prune_bottleneck(currentTensor, prevOutput, rindexes)\n",
    "\n",
    "        prevIndex = i\n",
    "        prevTensor = currentTensor\n",
    "\n",
    "    #Prune RoI pooling and FC classifiers\n",
    "    b = pruneNextLayer(fasterRCNN.RCNN_rpn.RPN_Conv, prevOutput, rindexes)\n",
    "    b, p, r = prune_bottleneck(fasterRCNN.RCNN_top[0], prevOutput, rindexes)\n",
    "    fc = pruneNextLayer(fasterRCNN.RCNN_bbox_pred, p, r)\n",
    "    cls_score = pruneNextLayer(fasterRCNN.RCNN_cls_score, p, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:34.095781Z",
     "start_time": "2018-05-14T20:23:20.460154Z"
    }
   },
   "outputs": [],
   "source": [
    "load_model_path = \"pth_train_dir/vgg16/finished_models/vgg16_test_2_60000.pth\"\n",
    "fasterRCNN = vgg16(holly_classes, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()\n",
    "checkpoint = torch.load(load_model_path)\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:34.263187Z",
     "start_time": "2018-05-14T20:23:34.097909Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def reload_model(weightsPTH):\n",
    "    fasterRCNN = vgg16(holly_classes, pretrained=False, class_agnostic=False)\n",
    "    fasterRCNN.create_architecture()\n",
    "    checkpoint = torch.load(weightsPTH)\n",
    "    fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "    fasterRCNN.cuda()\n",
    "    if 'pooling_mode' in checkpoint.keys():\n",
    "        cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "    return fasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:34.383437Z",
     "start_time": "2018-05-14T20:23:34.265911Z"
    }
   },
   "outputs": [],
   "source": [
    "if isCuda:\n",
    "    fasterRCNN.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:35.110570Z",
     "start_time": "2018-05-14T20:23:34.385295Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def evalNetwork(modelFR, out=False):\n",
    "    modelFR.eval()\n",
    "    \n",
    "    cfg.TRAIN.USE_FLIPPED = False\n",
    "    imdb_name = \"holly_test\"\n",
    "    imdb, roidb, ratio_list, ratio_index = combined_roidb(imdb_name, False)\n",
    "    imdb.competition_mode(on=True)\n",
    "    \n",
    "    save_name = 'faster_rcnn_10'\n",
    "    num_images = len(imdb.image_index)\n",
    "    all_boxes = [[[] for _ in range(num_images)]\n",
    "               for _ in range(imdb.num_classes)]\n",
    "\n",
    "    output_dir = get_output_dir(imdb, save_name)\n",
    "\n",
    "    dataset = roibatchLoader(roidb, ratio_list, ratio_index, 1, \\\n",
    "                            imdb.num_classes, training=False, normalize = False)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1,\n",
    "                                shuffle=False, num_workers=0,\n",
    "                                pin_memory=True)\n",
    "    output_dir = \"/tmp/outTest\"\n",
    "    vis = False\n",
    "    thresh = 0.0\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    num_images = len(imdb.image_index)\n",
    "\n",
    "    start = time.time()\n",
    "    max_per_image = 100\n",
    "\n",
    "    im_dataC = torch.FloatTensor(1)\n",
    "    im_infoC = torch.FloatTensor(1)\n",
    "    num_boxesC = torch.LongTensor(1)\n",
    "    gt_boxesC = torch.FloatTensor(1)\n",
    "\n",
    "    # ship to cuda\n",
    "    im_dataC = make_cuda(isCuda, im_dataC)\n",
    "    im_infoC = make_cuda(isCuda, im_infoC)\n",
    "    num_boxesC = make_cuda(isCuda, num_boxesC)\n",
    "    gt_boxesC = make_cuda(isCuda, gt_boxesC)\n",
    "\n",
    "      # make variable\n",
    "    im_dataC = Variable(im_dataC, volatile=True)\n",
    "    im_infoC = Variable(im_infoC, volatile=True)\n",
    "    num_boxesC = Variable(num_boxesC, volatile=True)\n",
    "    gt_boxesC = Variable(gt_boxesC, volatile=True)\n",
    "\n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    det_file = os.path.join(output_dir, 'detections.pkl')\n",
    "\n",
    "    empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))\n",
    "    for i in range(num_images):\n",
    "\n",
    "        data = next(data_iter)\n",
    "        im_dataC.data.resize_(data[0].size()).copy_(data[0])\n",
    "        im_infoC.data.resize_(data[1].size()).copy_(data[1])\n",
    "        gt_boxesC.data.resize_(data[2].size()).copy_(data[2])\n",
    "        num_boxesC.data.resize_(data[3].size()).copy_(data[3])\n",
    "\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "        rois_label = modelFR(im_dataC, im_infoC, gt_boxesC, num_boxesC)\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        if cfg.TEST.BBOX_REG:\n",
    "              # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "              # Optionally normalize targets by a precomputed mean and stdev\n",
    "                if False:#args.class_agnostic:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4)\n",
    "                else:\n",
    "                    box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                               + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                    box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_infoC.data, 1)\n",
    "        else:\n",
    "              # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "        pred_boxes /= data[1][0][2]\n",
    "\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "        if vis:\n",
    "            im = cv2.imread(imdb.image_path_at(i))\n",
    "            im2show = np.copy(im)\n",
    "        for j in range(1, imdb.num_classes):\n",
    "            inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "              # if there is det\n",
    "            if inds.numel() > 0:\n",
    "                cls_scores = scores[:,j][inds]\n",
    "                _, order = torch.sort(cls_scores, 0, True)\n",
    "                if False:#args.class_agnostic:\n",
    "                    cls_boxes = pred_boxes[inds, :]\n",
    "                else:\n",
    "                    cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "\n",
    "                cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "                # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "                cls_dets = cls_dets[order]\n",
    "                keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "                cls_dets = cls_dets[keep.view(-1).long()]\n",
    "                if vis:\n",
    "                    im2show = vis_detections(im2show, imdb.classes[j], cls_dets.cpu().numpy(), 0.3)\n",
    "                all_boxes[j][i] = cls_dets.cpu().numpy()\n",
    "            else:\n",
    "                all_boxes[j][i] = empty_array\n",
    "\n",
    "          # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "            image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                        for j in range(1, imdb.num_classes)])\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "                for j in range(1, imdb.num_classes):\n",
    "                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "          .format(i + 1, num_images, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if vis:\n",
    "            cv2.imwrite('result.png', im2show)\n",
    "            pdb.set_trace()\n",
    "\n",
    "    with open(det_file, 'wb+') as f:\n",
    "        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Evaluating detections')\n",
    "    ap = imdb.evaluate_detections(all_boxes, output_dir, out)\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:35.269665Z",
     "start_time": "2018-05-14T20:23:35.112544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43 54 53  2  9 62  7 32 61  1 63  3 40 40 38 41 51 26 48 56 16 24 15  4\n",
      " 18 25  9 14 46 10 31 50 37  9  5 49 43 27 36 41 48 38 19 16 12  7 54  2\n",
      " 21 46  4 29 42 18 47 10 36 50 52 47 46 48  0  7]\n",
      "[31 53 58 20 37 50 55 48 43  7 46 45 31 15 58 16 51 17 25 63 42 59  4 26\n",
      "  4 57 62  9 51 22  3 20  3 53 31 51 41 55 40  5 40 52 42 55 50 49  5 10\n",
      " 46 10 42 32 10  9  8  0 56  3 43  3 14 53 10 34]\n",
      "[ 18  79  91   9 108 117 106   1 113  18  25  48  59  41  66  16  49  10\n",
      "  41 111  83 104   0  83  37  27  45  86  51  31  16  23  84  15 105  50\n",
      "  12  28  56 103  92  84 112 123  68  54  54 127 103  29  15  62  71  75\n",
      "  18  84  16  78  26  33  12 102  73  22  98  70  28  72  29 109  42  83\n",
      "  25 119  52   4  68  80  15  16  21  45 125  52  11 124   2  41  65  69\n",
      "  58 105 121  88  48  32 104  44  27  32  52 126 111  78  12 126  58  83\n",
      "  42   4  81   5  29  19 107   8  88  81  64  29  65  33  11  92   2  82\n",
      "  68  94]\n",
      "[ 40   9 107  30 107 118  21  54  15  35  46  19  54  95  24  99  59  64\n",
      " 116  24 112  51  54 102 117 102  69  22 125  94  84  77  73  13  74 123\n",
      " 116  39  24  39 101  58  71  19  36  36 114 101  91  23  63  50 127 116\n",
      "  61   9  27  58  72  32  86  33  96 124   1  73  54  46  19  89  92  82\n",
      "  25  41 120  68  60 122  16  15 117  14  69  49  65  54  40 100  94  21\n",
      "   3  38  38  75  61 109  88 120 124  56  18  28  95  35   8  54 118  27\n",
      " 121  67  85  97  25   8  55  92  88  18 124  69 126  46  53 116 110  80\n",
      "  28   0]\n",
      "[ 82 194  66 173 120 253  88  59 198 216  32 148 148  40  23  55 158 132\n",
      " 184 103 185 240 218 122 110 151 246 111 123  54   8  51 143 245  83  63\n",
      "  39 121 108  76 245 143  78 177 244 225 163  13  16 244   0 220 162   1\n",
      "  56  38 113   6 153  99 108 252 174  68 197 237 174  41 112  72  36 143\n",
      " 190 150  64 190 249 229 149 178 235 176 212  72 234 218 202 242 134 176\n",
      "  11  66  58 238  22 140 147 126 109 117  47 238 118 105  43  77 193 220\n",
      " 183 234 241 166   6  40  99 160 255 201  89 103 130 204  91 130  25 229\n",
      "  32 232 153 184 232 200  56 144 173  62 207  96  82 227 237 230  16 203\n",
      " 124  54  15   0 245   3 163 162 211  54 255 209  36  96   6  83  31  89\n",
      " 155  88  25 113 175 165 252 246 124  16  78  15  82 107  18 198  93 169\n",
      " 100 117  14  85 117  99  85 112 237 255  85  78 160 124 178   3  65 124\n",
      " 120 199  63 174 180 248 218   4   9 134 201 124 117 169 218 144  64 174\n",
      " 145  95 139   6 238 236  91  82 249 225 143 179 249  72  69   9 131 181\n",
      " 148 242  27  74  15 171 195 104 131 215 217 114 220 241  13  55  79 128\n",
      "  38  13 203 101]\n",
      "[ 88 103 243 135  82 118 187 105 199 124 164 223 100   5 137  58 227  59\n",
      " 136 209 137  34 191 164 152 196  95  72  57   2  42   3 154  80  69 169\n",
      " 200 217   2 254 179  24 110 166 171 108 212 158 169  80 253 248 161  70\n",
      "  76 191   5  47  82  41 250 207  38 170 218  16  43 166 166  64 161  32\n",
      "  47 129  57  57 171  44 182 207  23 163  65 140 187  44 231 226 229 170\n",
      " 107  15 133  91 179 125 141  42  80  92 245  77  96  64  59  45 102 135\n",
      " 236 235  74  15 227 118  49 214 154 227  77 249  40 163 203  55 186 206\n",
      "  79  47  81  82 232 181 252  88 176   3 197 211  81 203 115 251 176  13\n",
      "  50  59 251  18 171 172 112 169 238 157  71 160 151 108 230 109 246 159\n",
      " 114  50 204 164 202  62 201  54 122  56  58  38 217  71  28 100 133  97\n",
      " 185 244 169 237 162 121  85  44  80   3 138 251 170  10 211 185 105  67\n",
      " 130  73 200 191 140 231 188 163  77 198 181 188 244 116 176  26 208 193\n",
      " 249 153 143 104  68  77 171 164  29   2 185 185 117 126 231  50 185  14\n",
      " 177 110 196 184 129  38 178  83 119  21  61 252 205  35 203 137 157 133\n",
      "  21 110 111  36]\n",
      "[246 232 125 253 239 231 146  22  44  66 165   7  47 185 141 208 217  61\n",
      " 195  86  73  51  37 146 193  47 128 220  44 240 173 107 130  64 230  74\n",
      " 123  60 254 186 165 182  73 125  60 255 212 156 219 190 167 158 228  51\n",
      " 208  62  58 127  15 113 171  83 134 180 185  58  89 157 128 103 252 116\n",
      "  15 247 174   9 121 163 149 104 130  48 196 193 201  22 119 126 177 176\n",
      "  12 211  75 103  95  85 246 102 174 233  39 171 241  78  76  70 134  76\n",
      " 170  91 240  87  46 222 142 140 170  49 154 224 109  93  55  89  28 179\n",
      " 190 246   0 178 215 104  97 221 138 207 230 201  47 251  45 231  44 178\n",
      "  89  26  30 148  26 166 116 121 142  49 146 245  44   5 222  80  18  27\n",
      " 233  43  25  34   5 163  23  96 200 171 179  76  55 109 154 218  89 101\n",
      " 158 177  10  50 123 216 193 252 114  39 222 110 117 145 132 241  36 108\n",
      "  54 100  61  50 149 213  68  66   0  81 182 232 225 197 143  49 106  40\n",
      " 219 218 197 240  64  43 178 142  14 198 193  21 179 147 185 195 162   3\n",
      " 127 221  60 199 218 211  72 210 138  20 248  99  51 109 247  86 100  37\n",
      " 231  97 243 114]\n",
      "[126 375 223 419  73  84  47  34 234 394  79 220 133 302 312 210 381 415\n",
      " 374 194 392 466 321 222 498 350  83 141 103 438 304   3 462 362   3 450\n",
      " 425 438 164  14 492 196 254 121  60 462 474 384 134 191 340 425  70 242\n",
      "  65 270 458 266 477  58 228 130 213 491 341  28 216 501 389  30 106  80\n",
      " 397   3 368  20 429 287 491 138  23  99  15 249 227 343 100 117 473  84\n",
      " 418 507 511 417 301 471 181 356 208 194 244 223 165  74 176 439 242 164\n",
      " 221 396 456 425 168 354  64 491 190   4 384 401 381 298 262 356 321 107\n",
      " 186  47 185 319 150 114 400  89 342   3 346 363 426 364 172 504  77 153\n",
      "  32 207 345  87 330  71  53 461 140 425  48 148  68 155 293 455 463 315\n",
      " 216 347 428 283  79 478 488  41 361 269  20 224   3 153 502  29 399 327\n",
      "  87 181 403 509 247  13 120 130 312  82 278 269 386  24 368 452  25 120\n",
      "  40 496  39 450 339 410  88  59 226 511 307 427  28 435 201 384 102  67\n",
      " 111 219 183 183 139 178 427 339 192 309   0 309 178 344 443 149 238 465\n",
      "  61 363 234 273 434 363 457 166 319  80 347  18 475  34 443 237 154 347\n",
      " 113 407 481 449 415  34 279 345 444 111 387  80 121 315 293 163 366 230\n",
      " 430 159 499 238 155 218 242 267 265 459 177  53 486 361  30  43 191 242\n",
      " 152  11 407 396 277 136  95 508   7 368  22 423 470 503 471 153 111 457\n",
      " 260 115 114 480 295 121 414 230 291 186  97 208  70 273 131  54 176 190\n",
      "  59 189 111 264 298 462 297 103 185 307  37 330 128  97 120 385  46  99\n",
      " 320 403 215 448 206  45 238  50 140 504  89 144 480   9 142 316 323 146\n",
      " 350 360 430 365 277 180 488  83 449 510 131 346 247 383 130  95 432 490\n",
      " 189 403 420 441   1 303  99 510 438  37  46 415  91  38 169 237 408 425\n",
      " 238 147 128 233 371 290 299 178 141 125  65 215 508 464 396 148 287 365\n",
      "  85 462  69  93 468  60 500 311 479 265 311   1 158  65 416 313   7 380\n",
      " 343 302 240 447 413  91 299 481 141 457 250 422 357 407 477 510 257 201\n",
      " 318 281 293 355 234 323 230  88 465 319 458 293 123 359 190 359 358 509\n",
      " 400 497 416 444 259  59 191 187 412  97 388 509 191 270 269 315 164 461\n",
      "  33 314 403 268 327 371  25 345 136 507 347 122 327 394 489 112  52 120\n",
      " 172 132  90  68 315 218 370 201]\n",
      "[215 164 371 199 220  43 451 189 219 414 376 354  41 409 301 304  37 402\n",
      " 502 439 334 107 479 318 475 494 309 390  18 165 358 312 232 273 253 208\n",
      " 423 153 398 101 343 126 129 119 212 415  11 329 446 413   0 281   2 411\n",
      " 186 510 100 259 255 167 398 375 135 389 406  83 216 464 301 234  74  85\n",
      " 453  13 358  85 450 215 120 163 188 165 475 482  92 348 272  85 419 163\n",
      " 194 293 146 317  72 217 404 370 152 360  61 306 183 140 127 269 311 487\n",
      " 146  59 400 108 263 383 127 390  20  23 119 217 219 319  75 503 252 215\n",
      " 290 467  55  11 136 188 455 281 376 103  77 113 263 220 368 383 489 374\n",
      " 149  27 144 263 336 243 160 136 240 301 141 149 293 253  76  95  34 319\n",
      " 330 345 349 142 132 317 392  43 333 493 294 300  61 116 237 327 288 297\n",
      "  73  38 478 361  23 428  23 423 201 369  25  76 326  95   2 377  54 355\n",
      "  53 121 459 436  19 188 399 313 293 374 482 222 464 263 503 263 331 145\n",
      " 273 137 180   7 430 508 278 420 310 306 197 448 280 381 221 352  92 137\n",
      " 214 154 213 432 218 259 147  48 410 389  19 242 223 155 428 132 495  29\n",
      " 400 219 348  44   8 363  73 401 149 108 175 289 354 306 465  88 426 298\n",
      " 345  69 108 241 282 198 458  43  63 299 381 192 229 230  18 159 502 341\n",
      " 402 492 167 304  30 277 452 288 324 437 389  62 437 284 490 141 365 204\n",
      "  66 143  78 220 338 306 301 141 421 399 360 257 225 466 250 135  55 452\n",
      " 180  18 175 437 141  59 412 312 345  28 383 463 202 484 419 215 368 377\n",
      " 210 407  64 370 416  18 123 477  46 160 103 274 501 118 422 435 338  80\n",
      " 466 458 435 109   4 252 414  26 346 315  54 394   2 243 369 183 390 325\n",
      " 245 495 134 365 481 107 367 498 473 273 248 118 180 356 243 113 314 237\n",
      "  12 118  19 341 462 336 180 470 396 392 344 410 134 154 459 301  97 422\n",
      " 241 248  26 445 283 191 212 204  82 125 416 358 202 156  20 171  62 394\n",
      " 264 274  62 249  53 222 264  16 441 488 354 134 324 431 247 172 369 105\n",
      " 180 263 444 233 267 365 373 147 376 314 393  51 445 323  62 229  58  64\n",
      " 249 477 457 308 285 115 436 211 350 183 278  96 306 291 211 239  83  59\n",
      " 409  24 235  97  69 135 314  53 110 140  74 239 396 210 358 143 354 462\n",
      " 289 224 228 419  43 216  19 125]\n",
      "[113  71   8 456 141 134 397 397 450 257 480 351 318 501  45 299 387 170\n",
      " 168  93 427 284 495 313 175 172 468  23 318  53  72 312  21 449  90 441\n",
      " 169 443 440 499 392 358 389 274 396 110 371 132 289  90 502 204  78 333\n",
      " 143 113 142  81 407 347 392 365  72 483 447 391 419 503 137 210 108   2\n",
      " 460 153 206 509 269 459 426 369 207 313  73  98  75 108 169 482 227  36\n",
      " 509 302 324 333 379 111 305 361 474  44 259 276 293  21 378 383  41 116\n",
      "  41  40 392  14 247 254 162  80  66 118 385 456 488 283 150 449  67 442\n",
      " 171 323 242 242  33 218 426 369 338 390 298 301 202 316 420  74 470 482\n",
      " 160 328 461 394  56 388 497 486 144 333 188  76 109 449 252 364 392 347\n",
      " 493 509 262 394 415 461 424 331 441 488 218 244 281 358 275 131 189 340\n",
      " 480 168 451 354 413 299  84 500 140 318 303 295 342 276 210 364 206 486\n",
      " 351 443 133  31 477 238 470 112  48  71 174  70 164 503  46  42 394 348\n",
      " 294 176   2 156 172 116 268  85 189  26 284 211 470 282 177 149 293 373\n",
      " 440 355 352 504 273 125  91 425 256 488  76 416 405 189 489 370 185  85\n",
      " 452  89 141 476  94 253 268  80 121 363 447 435 227 235 475 208 404 498\n",
      " 285 359 184 340  75 241 173 230  47 357  10  49 270  74 184 468 133 413\n",
      " 332  11 484 284 430 372 256 497 448 218 262 113 135 383 273 344 470 302\n",
      " 491 306 177 277 112  63 376 459 334 390 459 180 433  34  35 258 335 440\n",
      " 157 229 407 154  66 473 356 442 384 194  89 140 239 147 238 469 425 107\n",
      " 491 460 104 269 293  34   8 440 290 395 351  54 507 257 180 212 106  67\n",
      " 389 433 185 264 181 286 359 491  69 387 181 431 333 107 146  73   5 251\n",
      " 321 487 492 461 453 184 380  72 484 340 447 100 416 325 218 332 448 329\n",
      " 274  97 444 447 416 442 273 178 218  72 125 220  77 473 486 478 298 176\n",
      " 435 231 351 385 124 218 280 301 320 195   6 136 376 378 196 218 228 235\n",
      " 366 325 151 232 503 254  86 302 122 475 289 176 389 116 382 434 311 405\n",
      " 269 227 412 287 349 226 277 489 293 403  50 109 351 299  90  85 481 127\n",
      " 114 406 213 253 350 225 149  90 372  58 309  98 387  81 167 345 494 340\n",
      " 357 194 123 401 256 156 154 427 410 510 453 448  50 120  39 382 238 508\n",
      "  37 209 222 388 126  80 307  86]\n",
      "[482 108 143  80 263 490  26 467 473 492 460  62 169  85  24 411  24 470\n",
      "  66 255 476 140  73  81 328 288  23 196 231 303 322  97 295 259 146 263\n",
      " 222 108   4 492  80 232 114  63 122 136  77  14 501 222 171 124 218 314\n",
      "  74 391 126 353 221 173  88 170 335 245 159 492 177 129 464 224   3 407\n",
      " 191 409 467 125 404 174 206 354  71 129 138 265 115 225 264 330  42 246\n",
      " 286 275 461 224 359 306 296  25 463 507 479 124 300 427 470  53  46 376\n",
      " 380 490  50 347 259 277 386 194 430 314 397   2 330 368 292 390 170 119\n",
      "  86 327 412 489 377 398  14 304 156 128  60 212 384  86   4 216 187 403\n",
      "  45 455 186  53 168  99  40  37 230  96  15 444 177 105 293 416 305  93\n",
      " 332 413 424 180  60 332 322 503 445 252 375 439 210 499 464 155 142  76\n",
      " 205 202 250 216 332 180 100  36 496 176 479 393 317 460 277 119 409 272\n",
      " 336 303  73 135 342 286 341 227 445 195 157 312 409 106 301 354 425 440\n",
      " 317 191  25 431 458  97  71 170 328 389  37 368 352 453 189 221 365 452\n",
      "  10 380 316 370 305  95 180 142 133 185   7 494 454  79  88 243 156 106\n",
      " 113 455  78  61 308 425 374  50 147  39 162  56  48 490 240 414 292 433\n",
      "  47 216 181 496 246 483  40 291 104 149  91 131 403 357 441 213  36 473\n",
      "  48 380 299 464 403 306 401 491 469 148 170 420  14 187 487 187 473 227\n",
      " 198 156  58 122 183 363 483 274  14  13 329 348 206  94 134 235 337 341\n",
      "  38 132  84 228 372 411 251 225  41 223  81 457  85 257 416 163 255 152\n",
      "  21 260 108 160 402 267 365 339 500  73 264 489 265 137 138 348  37 236\n",
      " 122  85 358 184 151 400 113 402 300 447  18 485 449  71 126 399 237 313\n",
      " 389 458 341   4 134 355 330  36 286 151  51  31 378 498 339 280 249 253\n",
      " 424 325  36 211 108 233 162  72 304  42 441 259 166 473 113 319 498 270\n",
      " 127 202  73 132 349 494 302 408  12 417 345 356  12 456 348 195  11 100\n",
      " 177 507  14  55 181 348 162 122 300  61  62 437 427  29 469  15 219 219\n",
      " 121  76 508 247 245 460 202 434  26  60 224 207 131 463 136 181  42  76\n",
      "  22 422 311  79 507 167 117 465 265 137 474 426  39  36  95 382 407 458\n",
      " 374 464 306 192  65 218 306 230 255 284 305 377 289 382 336 268  10 492\n",
      " 338 229 442 396  71 257 240 431]\n",
      "[125  40 164 135  72 443 127 142 200 486  70 351  51 370 190 252 182  81\n",
      " 171 319 294 127 157 311 334 175 460 140  66 398 337 216 379 178 493 504\n",
      " 205 119 356 183 479 248 269 265 340 148 162 306 384 450 172  47 242  15\n",
      " 160  83 176 135 357  28 149 437 391  58 410 415  72 403 283 335 495 472\n",
      "  46  65 199 377 214 455 372 192 413  75 401 302 505 239 490 184 280 118\n",
      " 174 380  61 329 203  42 337  81 424   6 162 445 125 135 385 268 283 422\n",
      "  27  34  82 142 495 244  86 323 313  91 504  13 314 380 183 385 476 441\n",
      "  81 414 195 334 323 378  73 238 214 480 485  39 413 468 349 397 397 212\n",
      " 468 119 329 133 398 135 428 498 243  34 145 409 225 240 361 200 101  88\n",
      "   7 398 271 431 242 188  67 463  63 469  42 387 144 379 381   1 507 190\n",
      " 161 262 442  37 254 310 175 280 113 198 127  16 462 223  89 120 467 329\n",
      " 169  17 222 100 347 248 481 158 358  75  65 115 426  29 501  45 268  32\n",
      " 446 460 503 135  89 132 120 159 488 410 192 250 189 302 216 178  55 218\n",
      " 490  50 300 145  18  28 375 459 438 415  83 118 224  47 215 332 141 486\n",
      " 269 162 362 164 499 475 299 214 284 325  90 371  18  58 121 194 482 254\n",
      "  62 308 249 150 200 488 475 459 461 137 246 495 222 316 297 304  30 251\n",
      " 449 101  36 228 233 192  31 192  44 352  62 102 130 240 243 335 288   1\n",
      " 285 436 176 418 234  69 309  67  54 473 475 188 345 317 170  43 287 502\n",
      "  60 125 272 391   2 415 185 318 413 263 453 446 322 457 400  97 179 407\n",
      " 216 497 456 403 406 475 289  30   7 150 120 418 364 444 187 155  18 227\n",
      " 454 499 252 503 414 488 263 238 501  71 125 498  88  84 287 419 496 459\n",
      " 334 150 329  26 280 207 376 137 234 253   0 315 126 300 491 272 480 325\n",
      " 270 480 179 439 315  50 444 126  80  73 103 427 230 256 245 256 372 444\n",
      "  29  98 387 322  38 450 488 273 181 296 263 326  52  60 269 357  73 223\n",
      " 223  16 280 429 229  24 456 174 230 502 193 506 176  35 378 348 107 343\n",
      " 287 380 265 123 177 452 330  45  42  10  46   6  46 445 209 128 373 274\n",
      " 136 283 348 394 469 389 244 181 205 194 386 468 293 339 228 188 336 181\n",
      "  47 381 333 276 193  81 422  43 350 163  69 218 145 437 502 341 341   0\n",
      " 452 433  55 175 139 487 278 189]\n"
     ]
    }
   ],
   "source": [
    "prevOutput = 3\n",
    "prevTensor = None\n",
    "compressionRate = 70\n",
    "for i, e in enumerate(fasterRCNN.RCNN_base):\n",
    "    if \"Conv\" in str(e) and i != 28:\n",
    "        if prevOutput != e.in_channels:\n",
    "            e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        index = np.random.randint(0, e.weight.shape[0], size=e.weight.shape[0])\n",
    "        print(index)\n",
    "        num2Removes = int(index.shape[0] * compressionRate / 100.)\n",
    "        _, rindexes = pruneConvWithIndexes(e, index[num2Removes:])\n",
    "        pruneCBias(e, rindexes)\n",
    "        prevOutput = e.out_channels\n",
    "        prevTensor = e\n",
    "    if i == 28:\n",
    "        e = pruneNextLayer(e, prevOutput, rindexes)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:35.289654Z",
     "start_time": "2018-05-14T20:23:35.271589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       "  (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace)\n",
       "  (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (5): Conv2d(20, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(39, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace)\n",
       "  (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (10): Conv2d(39, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(77, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace)\n",
       "  (14): Conv2d(77, 77, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace)\n",
       "  (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (17): Conv2d(77, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace)\n",
       "  (19): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace)\n",
       "  (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1), ceil_mode=False)\n",
       "  (24): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace)\n",
       "  (26): Conv2d(154, 154, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace)\n",
       "  (28): Conv2d(154, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN.RCNN_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T02:04:07.742509Z",
     "start_time": "2018-05-08T02:04:02.574447Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_70_random.p')\n",
    "torch.save(fasterRCNN, save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-21T19:04:54.442518Z",
     "start_time": "2018-04-21T19:04:48.900120Z"
    }
   },
   "outputs": [],
   "source": [
    "save_name_model = os.path.join(\"./pth_train_dir/vgg16/finished_models/\", 'pruned_50_entropy.p')\n",
    "fasterRCNN = torch.load(save_name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-08T18:31:37.744246Z",
     "start_time": "2018-05-08T18:31:36.578155Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pth_train_dir/vgg16/pruned_random_70/vgg16/holly/faster_rcnn_1_2_10000.pth\")\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:23:36.603938Z",
     "start_time": "2018-05-14T20:23:35.291431Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pth_train_dir/vgg16/pruned_eff_70/vgg16/holly/faster_rcnn_1_1_200000.pth\")\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:24:48.417552Z",
     "start_time": "2018-05-14T20:24:21.898978Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(fasterRCNN, \"pth_train_dir/vgg16/finished_models/pruned_70_eff_good.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:10:30.869771Z",
     "start_time": "2018-05-14T20:02:57.195433Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_detect: 2/1293 0.098s 0.002s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.060s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7418562571885307"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:31:53.778862Z",
     "start_time": "2018-05-14T20:24:52.381203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `HF__test` for training\n",
      "Set proposal method: gt\n",
      "Preparing training data...\n",
      "HF__test gt roidb loaded from /export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/data/cache/HF__test_gt_roidb.pkl\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/rpn/rpn.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im_detect: 3/1293 0.049s 0.001s   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/vision/lethanh/workspace/faster-rcnn.pytorch/lib/model/faster_rcnn/faster_rcnn.py:97: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections0.041s 0.001s   \n",
      "Writing head VOC results file\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7418562571885307"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasterRCNN = torch.load(\"pth_train_dir/vgg16/finished_models/pruned_70_eff_good.p\")\n",
    "evalNetwork(fasterRCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T14:32:05.669769Z",
     "start_time": "2018-04-05T14:32:05.209766Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/allFM.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e7b6415e38a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mallFM\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/allFM.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/allFM.p'"
     ]
    }
   ],
   "source": [
    "allFM =  pickle.load(open(\"/tmp/allFM.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
